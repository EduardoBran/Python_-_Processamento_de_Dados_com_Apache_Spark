{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df0e1e8f",
   "metadata": {},
   "source": [
    "# <span style=\"color: green; font-size: 40px; font-weight: bold;\"> Projeto (Usando PySpark e Spark SQL)</span>\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "\n",
    "# Analisando a Performance da Logística de Entrega com PySpark e Spark SQL\n",
    "\n",
    "<br>\n",
    "\n",
    "### Contexto\n",
    "\n",
    "Neste mini-projeto, vamos explorar um contexto de negócios na área de logística: a **análise da performance de entregas de uma transportadora**. O projeto será desenvolvido desde a concepção do problema de negócio até a entrega de insights extraídos dos dados utilizando ferramentas comuns de análise de dados no dia a dia de um Cientista de Dados. Utilizaremos o Apache Spark SQL para realizar consultas e análises nos dados, com foco em agregação, funções Window e parse de data.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "O objetivo deste mini-projeto é **analisar um conjunto de dados de uma transportadora responsável por entregas de produtos**. Utilizaremos o Spark SQL para extrair insights e compreender como está a performance da logística de entrega da empresa. As soluções para cada pergunta de negócio serão apresentadas em Linguagem SQL e com o uso de funções do Spark SQL.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Pergunta de Negócio Principal\n",
    "\n",
    "> \"Como podemos analisar a performance de entregas dos veículos de uma transportadora usando dados de horários de entregas?\"\n",
    "\n",
    "<br>\n",
    "\n",
    "### Entregável\n",
    "\n",
    "O entregável deste mini-projeto será uma **série de análises e insights sobre a performance de entregas de uma transportadora**. As análises serão realizadas utilizando o Spark SQL em um conjunto de dados fictício que simula os registros de entregas. O processo incluirá a concepção do problema de negócio, preparação dos dados e extração de insights.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Sobre o Conjunto de Dados\n",
    "\n",
    "Os dados utilizados neste mini-projeto são fictícios e representam os registros de entregas de uma transportadora. Cada registro contém o ID do veículo, a entrega realizada e o horário da entrega. O dataset é pequeno para permitir a realização rápida das consultas e demonstração de diversas funções e notações do Spark SQL.\n",
    "\n",
    "<br>\n",
    "\n",
    "O dataset possui 3 colunas:\n",
    "\n",
    "<br>\n",
    "\n",
    "<table border=\"2\">\n",
    "  <tr>\n",
    "    <th style=\"text-align: center; font-size: 16px;\">Nome da Coluna</th>\n",
    "    <th style=\"text-align: center; font-size: 16px;\">Tipo de Dado</th>\n",
    "    <th style=\"text-align: center; font-size: 16px;\">Descrição</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>id_veiculo</td>\n",
    "    <td>integer</td>\n",
    "    <td>Identificação do veículo que realizou a entrega.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>entrega</td>\n",
    "    <td>string</td>\n",
    "    <td>Nome da entrega realizada.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>horario</td>\n",
    "    <td>string</td>\n",
    "    <td>Horário em que a entrega foi realizada (formato HH:MMa).</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<br> <br> <br>\n",
    "\n",
    "# Importando Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1253391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Biblioteca principal do Apache Spark para processamento distribuído de grandes volumes de dados.\n",
    "import pyspark\n",
    "\n",
    "# Cria e gerencia a conexão com o cluster Spark.\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Ponto de entrada para criar DataFrames e utilizar funcionalidades do Spark SQL.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Cria especificações de janela para realizar operações como funções de ranking e agregação sobre partições \n",
    "# de dados.\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Refere-se a uma coluna em um DataFrame, permitindo manipulação de colunas.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Atribui números únicos a linhas dentro de uma partição, com base na ordem especificada.\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Função de janela que retorna o valor da próxima linha em relação à linha atual dentro de uma partição.\n",
    "from pyspark.sql.functions import lead  \n",
    "\n",
    "# Funções agregadas que calculam o valor mínimo e máximo de uma coluna, respectivamente.\n",
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "# Converte uma string de data/hora para um formato de timestamp Unix.\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "# Biblioteca para cálculos numéricos e manipulação de arrays\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c985b686",
   "metadata": {},
   "source": [
    "<br> <br>\n",
    "\n",
    "# <span style=\"color: green; font-size: 38px; font-weight: bold;\">Preparando o Ambiente Spark</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9406b3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.13:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Mini-Projeto4</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2b1c45cee0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definindo semente aleatória (seed) para reprodutibilidade do notebook\n",
    "rnd_seed = 23\n",
    "np.random.seed = rnd_seed\n",
    "np.random.set_state = rnd_seed\n",
    "\n",
    "# Se houver uma sessão Spark ativa, encerre-a\n",
    "if 'sc' in globals():\n",
    "    sc.stop()\n",
    "\n",
    "if 'spark' in globals():\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "# Criando o Spark Context\n",
    "conf = SparkConf().setAppName(\"Mini-Projeto4\") \\\n",
    "                  .set(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "                  .set(\"spark.executor.heartbeatInterval\", \"20s\") \\\n",
    "                  .set(\"spark.eventLog.enabled\", \"false\") \\\n",
    "                  .set(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "                  .set(\"spark.sql.debug.maxToStringFields\", \"100\") \\\n",
    "                  .set(\"spark.executor.memory\", \"4g\") \\\n",
    "                  .set(\"spark.driver.memory\", \"4g\") \\\n",
    "                  .set(\"spark.driver.maxResultSize\", \"2g\")  # Configuração adicional para limitar o tamanho do resultado\n",
    "\n",
    "# Criar o Spark Context e a Spark Session\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Ajustar o nível de log para ERROR\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Configurar log4j para suprimir avisos (deixar como comentário e volta ao normal)\n",
    "log4j_logger = sc._jvm.org.apache.log4j\n",
    "log4j_logger.LogManager.getLogger(\"org\").setLevel(log4j_logger.Level.ERROR)\n",
    "log4j_logger.LogManager.getLogger(\"akka\").setLevel(log4j_logger.Level.ERROR)\n",
    "\n",
    "# Visualizar o objeto spark_session\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb531be",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <span style=\"color: green; font-size: 38px; font-weight: bold;\">Carregando os Dados</span>\n",
    "\n",
    "- Vamos carregar os Dados diretamente como **Dataframe do Spark** pois **não vamos fazer análise exploratória de dados**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06b4b183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       298|Entrega 1|  7:58a|\n",
      "|       298|Entrega 2|  8:04a|\n",
      "|       298|Entrega 3|  8:17a|\n",
      "|       298|Entrega 4|  8:28a|\n",
      "|       298|Entrega 5|  8:33a|\n",
      "+----------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nome do arquivo\n",
    "arquivo = 'projeto04/dados/dataset.txt'\n",
    "\n",
    "# Carrega como dataframe do Spark\n",
    "df = spark.read.csv(arquivo, header = True)\n",
    "\n",
    "# Tipo\n",
    "print(type(df))\n",
    "\n",
    "# Visualiza primeiras 5 linhas\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca8adea",
   "metadata": {
    "id": "J1P0HuTRuN2O"
   },
   "source": [
    "<br> <br>\n",
    "\n",
    "# Criando Tabela Temporária\n",
    "\n",
    "- Criamos uma tabela temporária para executar consultas SQL nos dados. A tabela temporária existe somente nesta sessão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d39b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria tabela temporária\n",
    "df.createOrReplaceTempView(\"tb_logistica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd530182",
   "metadata": {
    "id": "fd-P7fEg4s5q"
   },
   "source": [
    "<br>\n",
    "\n",
    "## Executando Queries SQL\n",
    "\n",
    "- Vamos através de Queries visualizar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fae71611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  col_name|\n",
      "+----------+\n",
      "|id_veiculo|\n",
      "|   entrega|\n",
      "|   horario|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificando nome das colunas da tabela\n",
    "spark.sql(\"SHOW COLUMNS FROM tb_logistica\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b0fe2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|  col_name|data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|id_veiculo|   string|   NULL|\n",
      "|   entrega|   string|   NULL|\n",
      "|   horario|   string|   NULL|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Descreve a tabela para visualizar os tipos de dados das colunas\n",
    "spark.sql(\"DESCRIBE TABLE tb_logistica\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "228a53b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       298|Entrega 1|  7:58a|\n",
      "|       298|Entrega 2|  8:04a|\n",
      "|       298|Entrega 3|  8:17a|\n",
      "|       298|Entrega 4|  8:28a|\n",
      "|       298|Entrega 5|  8:33a|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizando os 5 primeiros registros\n",
    "spark.sql(\"SELECT * FROM tb_logistica LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ca98ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|  col_name|data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|id_veiculo|   string|   NULL|\n",
      "|   entrega|   string|   NULL|\n",
      "|   horario|   string|   NULL|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe da tabela (retornará null por causa do tipo dos dados das colunas)\n",
    "spark.sql(\"DESCRIBE tb_logistica\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53da3519",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Queries SQL x Dot Notation no Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c994eabf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de319d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64726bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b7d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea47d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3593125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546f99a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466bff9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
