{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b009f1",
   "metadata": {},
   "source": [
    "# <span style=\"color: green; font-size: 40px; font-weight: bold;\">Apache Spark</span>\n",
    "\n",
    "\n",
    "<br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c956ed",
   "metadata": {},
   "source": [
    "# O que é ?\n",
    "\n",
    "- É um <i>framework de processamento de dados distribuído de uso geral</i> e pode ser usado como <i>um framework de análise unificada extremamente rápido para Big Data e Machine Learning</i>.\n",
    "\n",
    "<br>\n",
    "\n",
    "- O Apache Spark permite processar grandes volumes de dados mais rapidamente, dividindo o trabalho em partes e atribuindo essas partes aos recursos computacionais através de máquinas de um cluester de computadores.\n",
    "\n",
    "<br>\n",
    "\n",
    "# Por que Precisamos do Apache Spark?\n",
    "\n",
    "#### A pergunta será respondida através de um <i>Storytelling</i>.\n",
    "\n",
    "<br>\n",
    "\n",
    "Imagine que você tenha que criar um modelo de Machine Learning com uma massa de dados de aproximadamente 10 GB.\n",
    "\n",
    "Você pode usar o Scikit-learn para construir o modelo e realizar todo o processamento em uma única máquina.\n",
    "\n",
    "Caso o próximo volume de dados for de 100GB, talvez seja necessários aumentar o hardware da máquina, adicionando mais CPU, mais memória RAM e mais espaço em disco.\n",
    "\n",
    "<br>\n",
    "\n",
    "O **problema** é que uma única máquina apresenta **escalabilidade vertical**, que é o limite de hardware que você pode adicionar ao computador. **Ou seja**, estaremos limitados à capacidade de um processamento de uma única máquina.<br>\n",
    "\n",
    "#### Como resolver esse problema ?\n",
    "\n",
    "Neste caso poderíamos usar um cluster de computadores, um conjunto de máquinas que vão trabalhar juntas, aumentando assim a capacidade de processamento.\n",
    "\n",
    "Em clusters, temos a escabilidade vertical (aumentando o hardware de cada máquina do cluster), mas temos também a escalabilidade horizontal, que é aumentar o número de máquinas no cluster. O problema agora poderia ser de limitação física para comportar todas essas novas máquinas e com isso teríamos um **outro problema**.\n",
    "\n",
    "O **problema** é que para executar um software em um ambiente de cluster de computadores, o software deve ser capaz de realizar processamento distribuído, ou seja, dividir uma tarefa de processamento em sub-tarefas, enviálas para as máquinas do cluster, coletar as respostas, juntar tudo e entregar o resultado.\n",
    "\n",
    "Toda essa gestão de processamento é feito via software, e por isso precisaríamos de um software que seja capaz de trabalhar de maneira distribuída e o **Scikit-learn não é um framework para procssamento distribuído**, é um framework sequencial.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### E agora por precisamos mesmo do Apache Spark ?\n",
    "\n",
    "> O **Apache Spark** é exatamente para realizar o processamento distribuído e em paralelo.\n",
    "\n",
    "<br>\n",
    "\n",
    "Podemos usar o Apache Spark em uma única máquina (como será feito aqui) ou em um cluster de 6.000 máquinas. A formo como construímos o processo de análise será a mesma!\n",
    "\n",
    "Além disso o Apacha Spark **oferece** bibliotecas poderosas para processamento de Machine Learning, processamento de dados estruturados com SQL, processamento de dados em tempo real com Streaming e processamento de grafos computacionais.\n",
    "\n",
    "O Apache Spark não tem um sistema de armazenamento, pois é um framework de processamento. mas pode ser usado em conjunto com um ambiente de armazenamento distribuído como o **Apache Hadoop HDFS**.\n",
    "\n",
    "Podemos usar o Apache Spark localmente ou na nuvem, como um pseudo-cluster (cluster de uma máquina só) para ambiente de desenvolvimento ou em um cluster de milhares de máquinas em um ambiente de produção.\n",
    "\n",
    "E ainda podemos trabalhar com linguagens Python, Java, Scala ou R\n",
    "\n",
    "<br>\n",
    "\n",
    "**Aqui** usaremos Linguagem Python e pseudo-cluster.\n",
    "\n",
    "\n",
    "<br> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33136783",
   "metadata": {},
   "source": [
    "# Ecossistema e Componentes do Apache Spark\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c44052a",
   "metadata": {},
   "source": [
    "> Na prática o Apache Spark é uma plataforma que possui uma śerie de componentes que podem ser usados em conjunto de acordo com o tipo do projeto. \n",
    "\n",
    "### Spark Core\n",
    "\n",
    "**Spark Core**: é o componente principal do ecossistema do Apache Spark (é o coração do sitema, chamado de motor), considerado mecanismo principal do Apache Spark. O Spark Core inclui o motor (engine) que realiza o processamento de dados distribuído e serve como a base para as outras bibliotecas do Apache Spark.\n",
    "\n",
    "O Spark Core fornece funcionalidades básicas de processamento de dados, incluindo:\n",
    "\n",
    "- Gerenciamento de tarefas de computação distribuída.\n",
    "- Operações básicas de leitura e escrita em fontes de dados.\n",
    "- Funcionalidades de cache e persistência de dados.\n",
    "- Suporte a várias linguagens de programação, como Python, Java, Scala e R.\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "### Principais Bibliotecas do Apache Spark\n",
    "\n",
    "> Acima do Spark Core, existem pelo menos quatro bibliotecas principais que expandem as funcionalidades do Spark, cada uma voltada para um tipo específico de processamento de dados:\n",
    "\n",
    "#### SparkSQL\n",
    "\n",
    "- Biblioteca para processamento de dados estruturados.\n",
    "- Permite executar consultas SQL em grandes conjuntos de dados de maneira distribuída.\n",
    "- Integra-se com várias fontes de dados como Hive, Avro, Parquet, entre outras.\n",
    "\n",
    "#### MLlib\n",
    "\n",
    "- Biblioteca de aprendizado de máquina.\n",
    "- Oferece uma ampla gama de algoritmos de machine learning, incluindo classificação, regressão, clustering e filtragem colaborativa.\n",
    "- Facilita a construção e implementação de modelos de machine learning em larga escala.\n",
    "\n",
    "#### GraphX\n",
    "\n",
    "- Biblioteca para processamento de grafos computacionais.\n",
    "- Permite a análise de grafos e redes sociais.\n",
    "- Oferece algoritmos para computação de grafos, como PageRank, Connected Components e Triangle Counting.\n",
    "\n",
    "#### Streaming\n",
    "\n",
    "- Biblioteca para processamento de dados em tempo real.\n",
    "- Permite o processamento contínuo de dados de fontes de streaming, como Kafka, Flume, e sockets TCP.\n",
    "- Suporta transformações de dados em tempo real e algoritmos de machine learning para streaming.\n",
    "\n",
    "#### Exemplificando\n",
    "\n",
    "- Para **construção de Modelos de Machine Learning** usamos o **MLlib**.\n",
    "- Para **capturar e processar dados em tempo real** usamos o **Streaming**.\n",
    "- Para **capturar dados em tempo real e aplicar Machine Learning** então usaremos **Streaming** juntamente co **MLlib**.\n",
    "- Para **executar uma consulta/query** em grande quantidade conjunto de dados em cluster, então usa o **SparkSQL**.\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "### APIs do Apache Spark\n",
    "\n",
    "> O Apache Spark suporta várias APIs de programação, permitindo que desenvolvedores trabalhem com a linguagem de sua preferência. As principais APIs são:\n",
    "\n",
    "- **Python (PySpark)**: Facilita a integração com bibliotecas Python populares, como pandas e NumPy.\n",
    "- **R**: Usada principalmente para análises estatísticas e gráficos.\n",
    "- **Java**: Ideal para desenvolvedores que trabalham em ecossistemas Java.\n",
    "- **Scala**: A linguagem nativa do Apache Spark, oferece alta performance.\n",
    "- **SQL**: Permite consultas SQL em dados estruturados, aproveitando a familiaridade com SQL.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Exemplificando\n",
    "\n",
    "Em um projeto, podemos trabalhar com a API **Python (PySpark)** e usar pelo menos uma biblioteca principal do Apache Spark. O uso de PySpark permitirá que aproveitemos a familiaridade com Python, enquanto exploramos as poderosas funcionalidades oferecidas pelas bibliotecas do Spark, como SparkSQL e MLlib, para processamento de dados e machine learning.\n",
    "\n",
    "Por exemplo, podemos usar a API PySpark para carregar dados, transformá-los e executar consultas SQL com SparkSQL, além de aplicar algoritmos de machine learning usando MLlib.\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "### Armazenamento no Apache Spark\n",
    "\n",
    "> O Apache Spark é um framework de processamento de dados que pode se integrar com diversas opções de armazenamento. Dependendo das necessidades do projeto e da infraestrutura disponível, você pode escolher entre diferentes sistemas de armazenamento. Abaixo estão algumas das opções mais comuns:\n",
    "\n",
    "#### Armazenamento Local\n",
    "\n",
    "- **Descrição**: Refere-se ao armazenamento de dados no sistema de arquivos local da máquina onde o Spark está sendo executado.\n",
    "- **Uso**: Útil para desenvolvimento e testes em pequenos volumes de dados.\n",
    "- **Limitações**: Não é escalável e não oferece tolerância a falhas.\n",
    "\n",
    "#### HDFS (Hadoop Distributed File System)\n",
    "\n",
    "- **Descrição**: Sistema de arquivos distribuído projetado para rodar em hardware comum. HDFS é uma parte central do Apache Hadoop.\n",
    "- **Uso**: Ideal para grandes volumes de dados distribuídos em um cluster. Oferece alta disponibilidade e tolerância a falhas.\n",
    "- **Vantagens**: Escalável, tolerante a falhas, e integrado nativamente ao ecossistema Hadoop.\n",
    "\n",
    "#### RDBMS (Relational Database Management System)\n",
    "\n",
    "- **Descrição**: Bancos de dados relacionais como MySQL, PostgreSQL, Oracle, etc.\n",
    "- **Uso**: Útil para dados estruturados que requerem transações ACID e suporte a SQL.\n",
    "- **Vantagens**: Oferece consistência transacional e suporte a consultas complexas.\n",
    "\n",
    "#### NoSQL\n",
    "\n",
    "- **Descrição**: Bancos de dados não relacionais como MongoDB, Cassandra, HBase, etc.\n",
    "- **Uso**: Ideal para dados semi-estruturados ou não estruturados, e aplicações que requerem alta escalabilidade e flexibilidade.\n",
    "- **Vantagens**: Suporta grande escalabilidade horizontal e oferece alta performance em leituras e gravações.\n",
    "\n",
    "#### AWS S3 (Amazon Simple Storage Service)\n",
    "\n",
    "- **Descrição**: Serviço de armazenamento em nuvem da Amazon, que fornece armazenamento de objetos através de uma interface de serviços web.\n",
    "- **Uso**: Excelente para armazenar grandes volumes de dados em um ambiente de nuvem, com alta durabilidade e disponibilidade.\n",
    "- **Vantagens**: Escalabilidade infinita, integração com outros serviços AWS, e custo-efetivo para armazenamento a longo prazo.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Gerenciamento do Apache Spark\n",
    "\n",
    "> O Apache Spark pode ser gerenciado de várias maneiras, dependendo do ambiente e das necessidades do projeto. As principais opções de gerenciamento incluem:\n",
    "\n",
    "#### Standalone\n",
    "\n",
    "- **Descrição**: Um gerenciador de cluster simples embutido no Spark. \n",
    "- **Uso**: Ideal para pequenos clusters e desenvolvimento.\n",
    "- **Vantagens**: Fácil de configurar e usar, não requer software adicional.\n",
    "\n",
    "#### YARN (Yet Another Resource Negotiator)\n",
    "\n",
    "- **Descrição**: Um gerenciador de recursos distribuído para o ecossistema Hadoop.\n",
    "- **Uso**: Integrado em clusters Hadoop para gerenciar recursos e executar aplicações.\n",
    "- **Vantagens**: Suporte robusto para Hadoop, escalabilidade e integração com HDFS.\n",
    "\n",
    "#### Kubernetes\n",
    "\n",
    "- **Descrição**: Um sistema de orquestração de contêineres.\n",
    "- **Uso**: Gerencia o deployment, a escalabilidade e a operação de aplicações em contêineres.\n",
    "- **Vantagens**: Alta escalabilidade, gerenciamento eficiente de contêineres e suporte a várias plataformas de nuvem.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Exemplificando novamente\n",
    "\n",
    "Em um projeto, podemos trabalhar com a API **Python (PySpark)** e usar pelo menos uma biblioteca principal do Apache Spark. O uso de PySpark permitirá que aproveitemos a familiaridade com Python, enquanto exploramos as poderosas funcionalidades oferecidas pelas bibliotecas do Spark, como SparkSQL e MLlib, para processamento de dados e machine learning.\n",
    "\n",
    "Por exemplo:\n",
    "\n",
    "- **Armazenamento de Dados**: Podemos carregar dados de um armazenamento local, HDFS, RDBMS, NoSQL ou AWS S3.\n",
    "- **Gerenciamento de Cluster**: O Spark pode ser gerenciado usando Standalone, YARN ou Kubernetes.\n",
    "- **Processamento e Análise**:\n",
    "  - Usar **PySpark** para carregar dados de uma fonte de armazenamento.\n",
    "  - Transformar os dados e executar consultas SQL com **SparkSQL**.\n",
    "  - Aplicar algoritmos de machine learning usando **MLlib**.\n",
    "  \n",
    "\n",
    "<br><br>\n",
    "\n",
    "# Quando PodemosUsar o Apache Spark?\n",
    "\n",
    "\n",
    "O Apache Spark podeser usado sempre que os requerimentos abaixo fizerem parte de um projeto de Ciência de Dados:\n",
    "\n",
    "<br>\n",
    "\n",
    "  - Integração de dados e ETL\n",
    "  - Análises interativase manipulação de grandes massas de dados\n",
    "  - Computação em batch de alta performance\n",
    "  - Análises avançadas e Machine Learning•\n",
    "  - Processamento de dados em tempo real\n",
    "  \n",
    "<br><br>\n",
    "\n",
    "# Conectando ao Cluster Spark\n",
    "\n",
    "Para trabalhar com o Apache Spark, primeiro devemos conectar no cluster Spark (seja um pseudo-cluster de uma única máquina, seja em um cluster de milhares de máquinas). A conexão é feita criando um SparkContext.\n",
    "\n",
    "<br>\n",
    "\n",
    "## SparkContext\n",
    "\n",
    "Um SparkContext representa a conexão com um cluster Spark e pode ser usado para criar RDDs (Datasets Distribuídos), acumuladores e variáveis de broadcast nesse cluster. Apenas um SparkContext deve estar ativo por JVM. Você deve parar o SparkContext ativo antes de criar um novo.\n",
    "\n",
    "<br>\n",
    "\n",
    "### SparkSession\n",
    "\n",
    "Com o contexto criado, podemos então criar uma sessão Spark. SparkSession é o ponto de entrada para o SparkSQL. Você cria uma SparkSession usando o método `Builder()`, que dá acesso à API do Builder que você usa para configurar a sessão.\n",
    "\n",
    "#### Funcionalidades do SparkSession\n",
    "\n",
    "Uma vez criada a sessão, SparkSession permite:\n",
    "- Criar um DataFrame\n",
    "- Criar um Dataset\n",
    "- Acessar serviços SparkSQL (por exemplo, ExperimentalMethods, ExecutionListenerManager, UDFRegistration)\n",
    "- Executar uma consulta SQL\n",
    "- Carregar uma tabela\n",
    "- Acessar a interface DataFrameReader para carregar um conjunto de dados do formato de sua escolha\n",
    "\n",
    "<br>\n",
    "\n",
    "## Múltiplas SparkSessions\n",
    "\n",
    "Você pode ter quantas SparkSessions quiser em um único aplicativo Spark. O caso de uso comum é manter as entidades relacionais separadas logicamente em catálogos por SparkSession.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Encerrando a SparkSession\n",
    "\n",
    "No final, você interrompe uma SparkSession usando o método `stop()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1b7787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d8359c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4e8e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30010e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36da43ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
