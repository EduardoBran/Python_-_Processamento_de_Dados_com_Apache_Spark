{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a101b1",
   "metadata": {},
   "source": [
    "# <span style=\"color: green; font-size: 40px; font-weight: bold;\"> Projeto (Regressão) </span>\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "# Prevendo a Cotação de Criptomoedas em Tempo Real com PySpark e Machine Learning\n",
    "\n",
    "<br>\n",
    "\n",
    "### Contexto\n",
    "\n",
    "Neste mini-projeto, vamos explorar um importante contexto de negócio na área de finanças: a **previsão da cotação de criptomoedas**. O projeto será desenvolvido desde a concepção do problema de negócio até a entrega de um modelo preditivo, utilizando ferramentas comuns de análise de dados no dia a dia de um Cientista de Dados. Apesar do foco ser em ferramentas de análise de dados, o projeto não serve como aconselhamento financeiro.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "O objetivo deste mini-projeto é **construir um modelo de Machine Learning capaz de prever a cotação de criptomoedas**. Usaremos dados históricos do Bitcoin para treinar o modelo. O Bitcoin, lançado em 2009 pelo anônimo Satoshi Nakamoto, é a criptomoeda mais antiga e conhecida. Servindo como meio descentralizado de troca digital, as transações de Bitcoin são verificadas e registradas em um livro público distribuído chamado Blockchain. O modelo deve ser capaz de prever a cotação do Bitcoin em tempo real a partir de novos dados de entrada. Este projeto pode ser estendido para outras criptomoedas ou instrumentos financeiros com dados de cotação disponíveis.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Pergunta de Negócio Principal\n",
    "\n",
    "> \"Como podemos prever a cotação futura do Bitcoin usando dados históricos?\"\n",
    "\n",
    "<br>\n",
    "\n",
    "### Entregável\n",
    "\n",
    "O entregável deste mini-projeto será um **modelo de Machine Learning treinado para prever a cotação do Bitcoin**. O modelo será desenvolvido utilizando dados históricos de cotação do Bitcoin e será capaz de fazer previsões em tempo real com base em novos dados de entrada. O processo incluirá a concepção do problema de negócio, preparação dos dados, desenvolvimento do modelo, e a entrega do modelo preditivo.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Sobre o Conjunto de Dados\n",
    "\n",
    "Os dados utilizados neste mini-projeto abrangem o período de 2011 a 2021. O arquivo CSV contém registros OHLC (Open, High, Low, Close) da cotação do Bitcoin, Volume em BTC e Volume na moeda (dólar).\n",
    "\n",
    "A última coluna indica o preço ponderado do Bitcoin. Os carimbos de data/hora (timestamp) estão em hora Unix. Timestamps sem atividade têm seus campos de dados preenchidos com NaNs. Se estiver faltando um carimbo de data/hora ou houver saltos, isso pode ser devido à inatividade da Exchange, inexistência da Exchange, ou outros erros técnicos na coleta dos dados.\n",
    "\n",
    "Optamos por não usar dados do ano de 2022 devido à sua natureza atípica, mas você pode incluir esses dados e treinar novamente o modelo se desejar.\n",
    "\n",
    "<br>\n",
    "\n",
    "Para este projeto, utilizaremos o conjunto de dados \"Bitcoin Historical Data\", que contém quase 5 milhões de linhas e informações detalhadas sobre as transações de Bitcoin ao longo do tempo. O conjunto de dados inclui dados sobre preços de abertura, fechamento, máximo e mínimo, volume de Bitcoins negociados, volume em moeda fiduciária e preço ponderado. Além disso, uma coluna adicional dateTime foi criada para converter os timestamps Unix em um formato de data e hora legível.\n",
    "\n",
    "<br>\n",
    "\n",
    "<table border=\"2\">\n",
    "  <tr>\n",
    "    <th style=\"text-align: center; font-size: 16px;\">Nome da Coluna</th>\n",
    "    <th style=\"text-align: center; font-size: 16px;\">Tipo de Dado</th>\n",
    "    <th style=\"text-align: center; font-size: 16px;\">Descrição</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Timestamp</td>\n",
    "    <td>integer</td>\n",
    "    <td>Representa o Unix timestamp, que é o número de segundos desde 1 de janeiro de 1970 (UTC).</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Open</td>\n",
    "    <td>double</td>\n",
    "    <td>Preço de abertura do Bitcoin no início do período de tempo.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>High</td>\n",
    "    <td>double</td>\n",
    "    <td>Preço mais alto do Bitcoin durante o período de tempo.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Low</td>\n",
    "    <td>double</td>\n",
    "    <td>Preço mais baixo do Bitcoin durante o período de tempo.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Close</td>\n",
    "    <td>double</td>\n",
    "    <td>Preço de fechamento do Bitcoin no final do período de tempo.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Volume_(BTC)</td>\n",
    "    <td>double</td>\n",
    "    <td>Volume total de Bitcoins negociados durante o período de tempo.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Volume_(Currency)</td>\n",
    "    <td>double</td>\n",
    "    <td>Volume total em moeda fiduciária (por exemplo, USD) das transações de Bitcoin durante o período.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Weighted_Price</td>\n",
    "    <td>double</td>\n",
    "    <td>Preço ponderado do Bitcoin, calculado com base nos preços e volumes das transações durante o período.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dateTime</td>\n",
    "    <td>string</td>\n",
    "    <td>Irá representar a data e hora no formato legível, será criada a partir do Unix timestamp. <b>(Nova Coluna)</b> </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<br> <br> <br>\n",
    "\n",
    "# Importando Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5196544",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module was compiled against NumPy C-API version 0x10 (NumPy 1.23) but the running NumPy has C-API version 0xf. Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module was compiled against NumPy C-API version 0x10 (NumPy 1.23) but the running NumPy has C-API version 0xf. Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem."
     ]
    }
   ],
   "source": [
    "### Imports\n",
    "\n",
    "# Importa o findspark e inicializa\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "## Bibliotecas de Manipulação e Análise de Dados\n",
    "\n",
    "import pandas as pd                      # Biblioteca para manipulação e análise de dados tabulares.\n",
    "import numpy as np                       # Biblioteca para cálculos numéricos e manipulação de arrays.\n",
    "\n",
    "\n",
    "## Bibliotecas de Visualização de Dados\n",
    "\n",
    "import seaborn as sns                    # Biblioteca para visualização de dados estatísticos.\n",
    "from matplotlib import pyplot as plt     # Biblioteca para criação de gráficos e visualizações.\n",
    "\n",
    "\n",
    "## Bibliotecas Principais do PySpark\n",
    "\n",
    "import pyspark                           # Biblioteca para processamento de dados em grande escala usando clusters.\n",
    "from pyspark import SparkConf            # Configuração e criação do contexto do Spark.\n",
    "from pyspark import SparkContext         # Configuração e criação do contexto do Spark.   \n",
    "from pyspark.sql import SparkSession     # Criação e manipulação de sessões e contextos SQL no Spark.\n",
    "from pyspark.sql import SQLContext       # Criação e manipulação de sessões e contextos SQL no Spark.\n",
    "from pyspark.sql.types import *          # Tipos de dados usados na criação de schemas de DataFrames no Spark.\n",
    "from pyspark.sql.functions import *      # Funções SQL usadas para manipulação e transformação de dados no Spark.\n",
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "\n",
    "## Bibliotecas de Machine Learning no PySpark\n",
    "\n",
    "from pyspark.ml.linalg import Vectors         # Estruturas de dados para manipulação de vetores na MLlib do Spark.\n",
    "from pyspark.ml.feature import StringIndexer  # Transformação de variáveis categóricas em numéricas.\n",
    "from pyspark.ml.regression import LinearRegression      # Algoritmo de regressão linear na MLlib do Spark.\n",
    "from pyspark.mllib.evaluation import RegressionMetrics  # Métricas de avaliação para modelos de regressão.\n",
    "from pyspark.ml.stat import Correlation                 # Cálculo de correlações entre colunas de DataFrames.\n",
    "from pyspark.ml.feature import MinMaxScaler             # Normalização dos dados para um intervalo específico.\n",
    "from pyspark.ml.feature import VectorAssembler   # Combinação de múltiplas colunas em uma única coluna de vetores.\n",
    "from pyspark.ml import Pipeline          # Construção de pipelines de ML que consistem em uma sequência de etapas.\n",
    "from pyspark.ml.tuning import ParamGridBuilder    # Ferramentas para construção de grids de parâmetros.\n",
    "from pyspark.ml.tuning import CrossValidator      # Ferramentas para construção de validação cruzada.\n",
    "from pyspark.ml.tuning import CrossValidatorModel # Ferramentas para construção de validação cruzada.\n",
    "from pyspark.ml.feature import StandardScaler     # Normalização de dados para ter média zero e variância unitária.\n",
    "from pyspark.ml.evaluation import RegressionEvaluator  # Avaliação de modelos de regressão utilizando \n",
    "                                                       # métricas como RMSE e R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8edee8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Formatação das saídas\n",
    "\n",
    "# Configura o Pandas para mostrar até 200 colunas ao exibir um DataFrame\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "# Configura o Pandas para mostrar até 400 caracteres por coluna ao exibir um DataFrame\n",
    "pd.set_option('display.max_colwidth', 400)\n",
    "\n",
    "# Configuramos matplotlib_axes_logger para exibir apenas mensagens de erro \n",
    "from matplotlib.axes._axes import _log as matplotlib_axes_logger\n",
    "matplotlib_axes_logger.setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a935ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Data Science Academy\n",
      "\n",
      "findspark : 2.0.1\n",
      "numpy     : 1.22.4\n",
      "sys       : 3.9.7 (default, Sep 16 2021, 13:09:58) \n",
      "[GCC 7.5.0]\n",
      "matplotlib: 3.4.3\n",
      "decimal   : 1.70\n",
      "pyspark   : 3.5.1\n",
      "seaborn   : 0.11.2\n",
      "pandas    : 1.3.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20637352",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Por que vamos usar o <i>PySpark</i> ao invés de utilizarmos somente <i>Linguagem Python</i>?\n",
    "\n",
    "> Antes de responder, podemos afirmar que SIM, este projeto poderia ser feito usando somente <i>Linguagem Python</i>.\n",
    "\n",
    "#### Então por que usar o PyStark?\n",
    "\n",
    "Como iremos ver a seguir, nosso conjunto de dados possui um tamanho de **317MB** e **quase 5 milhões de linhas**. Será que conseguiríamos processar esse volume de dados tão alto com Linguagem Python? Provavelmente não!\n",
    "\n",
    "<br>\n",
    "\n",
    "Portanto usaremos o **PySpark** pois o ele nos permite trabalhar em um ambiente distribuído. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c985b686",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Preparando o Ambiente Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecf12ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/11 18:37:12 WARN Utils: Your hostname, eduardo-Inspiron-15-3520 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface wlp0s20f3)\n",
      "24/07/11 18:37:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/11 18:37:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.13:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Mini-Projeto3</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5e3807b0a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definindo semente aleatória (seed) para reprodutibilidade do notebook\n",
    "rnd_seed = 23\n",
    "np.random.seed = rnd_seed\n",
    "np.random.set_state = rnd_seed\n",
    "\n",
    "# Criando o Spark Context\n",
    "conf = SparkConf().setAppName(\"Mini-Projeto3\") \\\n",
    "                  .set(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "                  .set(\"spark.executor.heartbeatInterval\", \"20s\") \\\n",
    "                  .set(\"spark.eventLog.enabled\", \"false\") \\\n",
    "                  .set(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "                  .set(\"spark.sql.debug.maxToStringFields\", \"100\")\n",
    "\n",
    "# Criar o Spark Context e a Spark Session\n",
    "sc = SparkContext(conf=conf)\n",
    "spark_session = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Ajustar o nível de log para ERROR\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Configurar log4j para suprimir avisos (deixar como comentário e volta ao normal)\n",
    "log4j_logger = sc._jvm.org.apache.log4j\n",
    "log4j_logger.LogManager.getLogger(\"org\").setLevel(log4j_logger.Level.ERROR)\n",
    "log4j_logger.LogManager.getLogger(\"akka\").setLevel(log4j_logger.Level.ERROR)\n",
    "\n",
    "# Visualizar o objeto spark_session\n",
    "spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2888cd2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bb531be",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Carregando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "080ff33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os dados a partir da sessão Spark\n",
    "df_spark = spark_session.read.csv('dados/dataset.csv', header = 'true', inferSchema = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bfab5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tipo do objeto\n",
    "type(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e8dec0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+----+-----+------------+-----------------+--------------+\n",
      "| Timestamp|Open|High| Low|Close|Volume_(BTC)|Volume_(Currency)|Weighted_Price|\n",
      "+----------+----+----+----+-----+------------+-----------------+--------------+\n",
      "|1325317920|4.39|4.39|4.39| 4.39|  0.45558087|     2.0000000193|          4.39|\n",
      "|1325319300| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|\n",
      "|1325319360| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|\n",
      "|1325319420| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|\n",
      "|1325319480| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|\n",
      "+----------+----+----+----+-----+------------+-----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualiza as 5 primeiras linhas dos dados\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b55cb176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Timestamp: integer (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume_(BTC): double (nullable = true)\n",
      " |-- Volume_(Currency): double (nullable = true)\n",
      " |-- Weighted_Price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualiza os metadados (schema) - similar ao info()\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d75197b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte o DataFrame do Spark para um DataFrame do Pandas (apenas se quiser)\n",
    "#df_pandas = df_spark.toPandas()\n",
    "\n",
    "# Visualiza os dados usando display()\n",
    "#display(df_pandas.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7328d1",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "# <span style=\"color: green; font-size: 34px; font-weight: bold;\"> Análise Exploratória de Dados </span>\n",
    "\n",
    "<br>\n",
    "\n",
    "### Análise Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b2ea7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " INFO \n",
      "\n",
      "\n",
      "root\n",
      " |-- Timestamp: integer (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume_(BTC): double (nullable = true)\n",
      " |-- Volume_(Currency): double (nullable = true)\n",
      " |-- Weighted_Price: double (nullable = true)\n",
      "\n",
      "\n",
      "\n",
      " ------------------------------------------------------------------------------------------ \n",
      "\n",
      "\n",
      "\n",
      " TOTAL DE VALORES NaN por Coluna \n",
      "\n",
      "+---------+-------+-------+-------+-------+------------+-----------------+--------------+\n",
      "|Timestamp|   Open|   High|    Low|  Close|Volume_(BTC)|Volume_(Currency)|Weighted_Price|\n",
      "+---------+-------+-------+-------+-------+------------+-----------------+--------------+\n",
      "|        0|1242831|1242831|1242831|1242831|     1242831|          1242831|       1242831|\n",
      "+---------+-------+-------+-------+-------+------------+-----------------+--------------+\n",
      "\n",
      "\n",
      "\n",
      "Existem valores ausentes: True\n",
      "\n",
      "Variáveis com valores ausentes: ['Open', 'High', 'Low', 'Close', 'Volume_(BTC)', 'Volume_(Currency)', 'Weighted_Price']\n",
      "\n",
      "Total de Linhas com Valores Ausentes: 1242831\n",
      "\n",
      "Porcentagem de Linhas Com Valor Ausente: 25.59%\n",
      "\n",
      "\n",
      "\n",
      "Existem valores duplicados: False\n",
      "\n",
      "Nenhuma variável possui valores duplicados.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analise_inicial(df):\n",
    "    # Exibir o schema do DataFrame\n",
    "    print('\\n\\n INFO \\n\\n')\n",
    "    df.printSchema()\n",
    "    print('\\n\\n ------------------------------------------------------------------------------------------ \\n\\n')\n",
    "\n",
    "    # Verificar valores ausentes em cada coluna\n",
    "    print('\\n TOTAL DE VALORES NaN por Coluna \\n')\n",
    "    valores_ausentes = df.select([count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in df.columns])\n",
    "    valores_ausentes.show()\n",
    "\n",
    "    # Verificar se existe alguma variável com valores ausentes\n",
    "    valores_ausentes_boolean = any(row[c] > 0 for row in valores_ausentes.collect() for c in df.columns)\n",
    "\n",
    "    # Nomes das variáveis com valores ausentes\n",
    "    variaveis_ausentes = [c for c in df.columns if df.filter(col(c).isNull() | isnan(c)).count() > 0]\n",
    "\n",
    "    # Número de linhas duplicadas\n",
    "    num_linhas_total = df.count()\n",
    "    num_linhas_distintas = df.distinct().count()\n",
    "    num_linhas_duplicadas = num_linhas_total - num_linhas_distintas\n",
    "\n",
    "    # Porcentagem de linhas duplicadas\n",
    "    porcentagem_linhas_duplicadas = (num_linhas_duplicadas / num_linhas_total) * 100\n",
    "\n",
    "    # Total de linhas com pelo menos um valor ausente\n",
    "    total_linhas_ausentes = df.filter(\n",
    "        \" OR \".join([f\"(`{c}` IS NULL OR isnan(`{c}`))\" for c in df.columns])\n",
    "    ).count()\n",
    "\n",
    "    # Porcentagem de linhas com pelo menos um valor ausente\n",
    "    porcentagem_linhas_ausentes = (total_linhas_ausentes / num_linhas_total) * 100\n",
    "\n",
    "    # Exibe o resultado\n",
    "    print(\"\\n\\nExistem valores ausentes:\", valores_ausentes_boolean)\n",
    "    if valores_ausentes_boolean:\n",
    "        print(\"\\nVariáveis com valores ausentes:\", variaveis_ausentes)\n",
    "        print(\"\\nTotal de Linhas com Valores Ausentes:\", total_linhas_ausentes)\n",
    "        print(\"\\nPorcentagem de Linhas Com Valor Ausente: {:.2f}%\".format(porcentagem_linhas_ausentes))\n",
    "    else:\n",
    "        print(\"\\nNenhuma variável possui valores ausentes.\")\n",
    "\n",
    "    print(\"\\n\\n\\nExistem valores duplicados:\", num_linhas_duplicadas > 0)\n",
    "    if num_linhas_duplicadas > 0:\n",
    "        print(\"\\nNúmero de Linhas Duplicadas:\", num_linhas_duplicadas)\n",
    "        print(\"\\nPorcentagem de Linhas Duplicadas: {:.2f}%\\n\".format(porcentagem_linhas_duplicadas))\n",
    "    else:\n",
    "        print(\"\\nNenhuma variável possui valores duplicados.\\n\")\n",
    "\n",
    "# Exemplo de uso da função com o DataFrame df_spark\n",
    "analise_inicial(df_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48c461",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# Analisando os Dados\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Visualizando Variáveis Categóricas e Numéricas\n",
    "\n",
    "- Todas as variáveis são numéricas neste dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af763322",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Analisando Todas as Variáveis\n",
    "\n",
    "#### Resumo Estatístico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f867296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------+-------+-------+-------+------------+-----------------+--------------+\n",
      "|summary|          Timestamp|   Open|   High|    Low|  Close|Volume_(BTC)|Volume_(Currency)|Weighted_Price|\n",
      "+-------+-------------------+-------+-------+-------+-------+------------+-----------------+--------------+\n",
      "|  count|            4856600|4856600|4856600|4856600|4856600|     4856600|          4856600|       4856600|\n",
      "|   mean|1.471324115749862E9|    NaN|    NaN|    NaN|    NaN|         NaN|              NaN|           NaN|\n",
      "| stddev|8.426671569710898E7|    NaN|    NaN|    NaN|    NaN|         NaN|              NaN|           NaN|\n",
      "|    min|         1325317920|    3.8|    3.8|    1.5|    1.5|         0.0|              0.0|           3.8|\n",
      "|    max|         1617148800|    NaN|    NaN|    NaN|    NaN|         NaN|              NaN|           NaN|\n",
      "+-------+-------------------+-------+-------+-------+-------+------------+-----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizar o resumo estatístico do DataFrame\n",
    "df_spark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437923fb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Limpeza nos Dados\n",
    "\n",
    "<br>\n",
    "\n",
    "### Tratando Valores Ausentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dde2ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c55ce53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9fc0b63",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# Features Engineering (se necessário)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Criando nova variável datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9629e446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be53e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3573ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dae3ed7b",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# Análise Exploratória\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## Verificando Correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a99a7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa5a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67129b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feaaf46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9f39a39",
   "metadata": {},
   "source": [
    "### Conclusão\n",
    "\n",
    "- Combinando a **análise de correlação** e a **análise dos gráficos**, a recomendação é"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd1d625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcec3682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3555dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8c97e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3afb56ad",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Pré-Processamento de Dados Para Construção de Modelos de Machine Learning\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "## Dividindo os dados em Dados de Treino e Dados de Teste\n",
    "- Nós **treinamos** o modelo com **dados de treino** e **avaliamos** o modelo com **dados de teste**.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7ea489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde6ef16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc78c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
