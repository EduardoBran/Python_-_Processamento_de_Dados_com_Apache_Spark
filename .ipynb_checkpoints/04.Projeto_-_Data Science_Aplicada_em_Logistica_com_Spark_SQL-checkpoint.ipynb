{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df0e1e8f",
   "metadata": {},
   "source": [
    "# <span style=\"color: green; font-size: 40px; font-weight: bold;\"> Projeto (Usando PySpark e Spark SQL)</span>\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "\n",
    "# Analisando a Performance da Logística de Entrega com PySpark e Spark SQL\n",
    "\n",
    "<br>\n",
    "\n",
    "### Contexto\n",
    "\n",
    "Neste mini-projeto, vamos explorar um contexto de negócios na área de logística: a **análise da performance de entregas de uma transportadora**. O projeto será desenvolvido desde a concepção do problema de negócio até a entrega de insights extraídos dos dados utilizando ferramentas comuns de análise de dados no dia a dia de um Cientista de Dados. Utilizaremos o Apache Spark SQL para realizar consultas e análises nos dados, com foco em agregação, funções Window e parse de data.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "O objetivo deste mini-projeto é **analisar um conjunto de dados de uma transportadora responsável por entregas de produtos**. Utilizaremos o Spark SQL para extrair insights e compreender como está a performance da logística de entrega da empresa. As soluções para cada pergunta de negócio serão apresentadas em Linguagem SQL e com o uso de funções do Spark SQL.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Pergunta de Negócio Principal\n",
    "\n",
    "> \"Como podemos analisar a performance de entregas dos veículos de uma transportadora usando dados de horários de entregas?\"\n",
    "\n",
    "<br>\n",
    "\n",
    "### Entregável\n",
    "\n",
    "O entregável deste mini-projeto será uma **série de análises e insights sobre a performance de entregas de uma transportadora**. As análises serão realizadas utilizando o Spark SQL em um conjunto de dados fictício que simula os registros de entregas. O processo incluirá a concepção do problema de negócio, preparação dos dados e extração de insights.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Sobre o Conjunto de Dados\n",
    "\n",
    "Os dados utilizados neste mini-projeto são fictícios e representam os registros de entregas de uma transportadora. Cada registro contém o ID do veículo, a entrega realizada e o horário da entrega. O dataset é pequeno para permitir a realização rápida das consultas e demonstração de diversas funções e notações do Spark SQL.\n",
    "\n",
    "<br>\n",
    "\n",
    "O dataset possui 3 colunas:\n",
    "\n",
    "<br>\n",
    "\n",
    "<table border=\"2\">\n",
    "  <tr>\n",
    "    <th style=\"text-align: center; font-size: 16px;\">Nome da Coluna</th>\n",
    "    <th style=\"text-align: center; font-size: 16px;\">Tipo de Dado</th>\n",
    "    <th style=\"text-align: center; font-size: 16px;\">Descrição</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>id_veiculo</td>\n",
    "    <td>integer</td>\n",
    "    <td>Identificação do veículo que realizou a entrega.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>entrega</td>\n",
    "    <td>string</td>\n",
    "    <td>Nome da entrega realizada.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>horario</td>\n",
    "    <td>string</td>\n",
    "    <td>Horário em que a entrega foi realizada (formato HH:MMa).</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<br> <br> <br>\n",
    "\n",
    "# Importando Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1253391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Biblioteca principal do Apache Spark para processamento distribuído de grandes volumes de dados.\n",
    "import pyspark\n",
    "\n",
    "# Cria e gerencia a conexão com o cluster Spark.\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Ponto de entrada para criar DataFrames e utilizar funcionalidades do Spark SQL.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Cria especificações de janela para realizar operações como funções de ranking e agregação sobre partições \n",
    "# de dados.\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Refere-se a uma coluna em um DataFrame, permitindo manipulação de colunas.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Atribui números únicos a linhas dentro de uma partição, com base na ordem especificada.\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Função de janela que retorna o valor da próxima linha em relação à linha atual dentro de uma partição.\n",
    "from pyspark.sql.functions import lead  \n",
    "\n",
    "# Funções agregadas que calculam o valor mínimo e máximo de uma coluna, respectivamente.\n",
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "# Converte uma string de data/hora para um formato de timestamp Unix.\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "# Biblioteca para cálculos numéricos e manipulação de arrays\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c985b686",
   "metadata": {},
   "source": [
    "<br> <br>\n",
    "\n",
    "# <span style=\"color: green; font-size: 38px; font-weight: bold;\">Preparando o Ambiente Spark</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9406b3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/30 12:18:46 WARN Utils: Your hostname, eduardo-Inspiron-15-3520 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface wlp0s20f3)\n",
      "24/07/30 12:18:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/30 12:18:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.13:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Mini-Projeto4</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3a9d05dfd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definindo semente aleatória (seed) para reprodutibilidade do notebook\n",
    "rnd_seed = 23\n",
    "np.random.seed = rnd_seed\n",
    "np.random.set_state = rnd_seed\n",
    "\n",
    "# Se houver uma sessão Spark ativa, encerre-a\n",
    "if 'sc' in globals():\n",
    "    sc.stop()\n",
    "\n",
    "if 'spark' in globals():\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "# Criando o Spark Context\n",
    "conf = SparkConf().setAppName(\"Mini-Projeto4\") \\\n",
    "                  .set(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "                  .set(\"spark.executor.heartbeatInterval\", \"20s\") \\\n",
    "                  .set(\"spark.eventLog.enabled\", \"false\") \\\n",
    "                  .set(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "                  .set(\"spark.sql.debug.maxToStringFields\", \"100\") \\\n",
    "                  .set(\"spark.executor.memory\", \"4g\") \\\n",
    "                  .set(\"spark.driver.memory\", \"4g\") \\\n",
    "                  .set(\"spark.driver.maxResultSize\", \"2g\")  # Configuração adicional para limitar o tamanho do resultado\n",
    "\n",
    "# Criar o Spark Context e a Spark Session\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Ajustar o nível de log para ERROR\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Configurar log4j para suprimir avisos (deixar como comentário e volta ao normal)\n",
    "log4j_logger = sc._jvm.org.apache.log4j\n",
    "log4j_logger.LogManager.getLogger(\"org\").setLevel(log4j_logger.Level.ERROR)\n",
    "log4j_logger.LogManager.getLogger(\"akka\").setLevel(log4j_logger.Level.ERROR)\n",
    "\n",
    "# Visualizar o objeto spark_session\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb531be",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <span style=\"color: green; font-size: 38px; font-weight: bold;\">Carregando os Dados</span>\n",
    "\n",
    "- Vamos carregar os Dados diretamente como **Dataframe do Spark** pois **não vamos fazer análise exploratória de dados**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06b4b183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'> \n",
      "\n",
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       298|Entrega 1|  7:58a|\n",
      "|       298|Entrega 2|  8:04a|\n",
      "|       298|Entrega 3|  8:17a|\n",
      "|       298|Entrega 4|  8:28a|\n",
      "|       298|Entrega 5|  8:33a|\n",
      "+----------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nome do arquivo\n",
    "arquivo = 'projeto04/dados/dataset.txt'\n",
    "\n",
    "# Carrega como dataframe do Spark\n",
    "df = spark.read.csv(arquivo, header = True)\n",
    "\n",
    "# Tipo\n",
    "print(type(df), '\\n')\n",
    "\n",
    "# Visualiza primeiras 5 linhas\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca8adea",
   "metadata": {
    "id": "J1P0HuTRuN2O"
   },
   "source": [
    "<br> <br>\n",
    "\n",
    "# Criando Tabela Temporária\n",
    "\n",
    "<br>\n",
    "\n",
    "- A criação de uma **tabela temporária** permite realizar consultas SQL nos dados, o que é útil para análises rápidas e interativas.\n",
    "- A tabela temporária existe somente nesta sessão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d39b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria tabela temporária\n",
    "df.createOrReplaceTempView(\"tb_logistica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd530182",
   "metadata": {
    "id": "fd-P7fEg4s5q"
   },
   "source": [
    "<br>\n",
    "\n",
    "## Executando Queries SQL\n",
    "\n",
    "- Vamos através de Queries visualizar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fae71611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  col_name|\n",
      "+----------+\n",
      "|id_veiculo|\n",
      "|   entrega|\n",
      "|   horario|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificando nome das colunas da tabela\n",
    "spark.sql(\"SHOW COLUMNS FROM tb_logistica\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b0fe2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|  col_name|data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|id_veiculo|   string|   NULL|\n",
      "|   entrega|   string|   NULL|\n",
      "|   horario|   string|   NULL|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Descreve a tabela para visualizar os tipos de dados das colunas\n",
    "spark.sql(\"DESCRIBE TABLE tb_logistica\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "228a53b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       298|Entrega 1|  7:58a|\n",
      "|       298|Entrega 2|  8:04a|\n",
      "|       298|Entrega 3|  8:17a|\n",
      "|       298|Entrega 4|  8:28a|\n",
      "|       298|Entrega 5|  8:33a|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizando os 5 primeiros registros\n",
    "spark.sql(\"SELECT * FROM tb_logistica LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ca98ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|  col_name|data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|id_veiculo|   string|   NULL|\n",
      "|   entrega|   string|   NULL|\n",
      "|   horario|   string|   NULL|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe da tabela (retornará null por causa do tipo dos dados das colunas)\n",
    "spark.sql(\"DESCRIBE tb_logistica\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53da3519",
   "metadata": {},
   "source": [
    "<br> <br>\n",
    "\n",
    "# Queries SQL x Dot Notation no Spark SQL\n",
    "\n",
    "<br>\n",
    "\n",
    "Utilizando <i>tabela temporária **tb_logistica</i>** criada na etapa anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "209a5212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|veiculo|  entrega|\n",
      "+-------+---------+\n",
      "|    298|Entrega 1|\n",
      "|    298|Entrega 2|\n",
      "|    298|Entrega 3|\n",
      "|    298|Entrega 4|\n",
      "|    298|Entrega 5|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query SQL\n",
    "spark.sql('SELECT id_veiculo AS veiculo, entrega FROM tb_logistica LIMIT 5').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bac53d",
   "metadata": {},
   "source": [
    "Utilizando <i>método **Dot Notattion**</i> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de319d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|veiculo|  entrega|\n",
      "+-------+---------+\n",
      "|    298|Entrega 1|\n",
      "|    298|Entrega 2|\n",
      "|    298|Entrega 3|\n",
      "|    298|Entrega 4|\n",
      "|    298|Entrega 5|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dot Notation\n",
    "df.select(col('id_veiculo').alias('veiculo'), 'entrega').limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0bafb2",
   "metadata": {},
   "source": [
    "### O que foi feito no código acima?\n",
    "\n",
    "O código acima mostra duas maneiras de selecionar e renomear colunas em um DataFrame do Spark e limitar o resultado a 5 registros. A primeira abordagem usa SQL e a segunda usa notação de ponto (dot notation) do PySpark.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Diferença entre Queries SQL e Dot Notation no Spark SQL\n",
    "\n",
    "#### 1. Queries SQL:\n",
    "- **Sintaxe SQL**: A abordagem usa sintaxe SQL padrão para manipulação de dados.\n",
    "- **Criação de Tabela Temporária**: Necessita que os dados sejam registrados como uma tabela temporária antes de executar consultas SQL. (criada na etapa anterior)\n",
    "- **Flexibilidade**: Familiar para aqueles que conhecem SQL, facilita a escrita de consultas complexas.\n",
    "\n",
    "#### 2. Dot Notation (Notação de Ponto):\n",
    "\n",
    "- **API do PySpark**: Usa a API de DataFrame do PySpark para manipulação de dados.\n",
    "- **Sem Necessidade de Tabela Temporária**: Operações são realizadas diretamente no DataFrame sem a necessidade de criar uma tabela temporária.\n",
    "- **Integração com Python**: Combina bem com outras bibliotecas e funcionalidades do Python.\n",
    "\n",
    "<br>\n",
    "\n",
    "Ambas as abordagens têm seus próprios méritos e a escolha entre elas pode depender da familiaridade do usuário com SQL ou APIs do PySpark e do tipo de operações que se deseja realizar.\n",
    "\n",
    "<br> <br> <br>\n",
    "\n",
    "# Usando Funções SQL do Spark SQL com Dot Notation\n",
    "\n",
    "<br>\n",
    "\n",
    "- A etapa seguinte enfatiza o uso de Dot Notation para manipulação de dados no Spark SQL.\n",
    "- As funções do Spark SQL são otimizadas para trabalhar em ambiente distribuído e podem oferecer melhor desempenho ao processar grandes conjuntos de dados. Essa abordagem é demonstrada por meio de vários exemplos que mostram como selecionar, renomear, acessar, filtrar e coletar dados diretamente no DataFrame do PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d1b7d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       298|Entrega 1|  7:58a|\n",
      "|       298|Entrega 2|  8:04a|\n",
      "+----------+---------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizando dataframe carregado anteriormente\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "970fd3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id_veiculo', 'entrega', 'horario']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Colunas do dataframe\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50699425",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Transformando em objeto Pandas (apenas para exemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3593125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> \n",
      "\n",
      "id_veiculo    object\n",
      "entrega       object\n",
      "horario       object\n",
      "dtype: object \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_veiculo</th>\n",
       "      <th>entrega</th>\n",
       "      <th>horario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>298</td>\n",
       "      <td>Entrega 1</td>\n",
       "      <td>7:58a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>298</td>\n",
       "      <td>Entrega 2</td>\n",
       "      <td>8:04a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_veiculo    entrega horario\n",
       "0        298  Entrega 1   7:58a\n",
       "1        298  Entrega 2   8:04a"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Podemos converter um Spark DataFrame para um Pandas DataFrame (e assim usar os métodos do Pandas)\n",
    "pandasDF = df.toPandas()\n",
    "\n",
    "# Tipo do objeto\n",
    "print(type(pandasDF), '\\n')\n",
    "\n",
    "# Tipo das colunas\n",
    "print(pandasDF.dtypes, '\\n')\n",
    "\n",
    "# Visualizando\n",
    "display(pandasDF.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06b11a0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Métodos para Manipulação de Dados no Spark SQL\n",
    "\n",
    "> Abaixo, serão apresentados diversos métodos de manipulação de dados no Spark SQL usando Dot Notation, incluindo seleções, filtragens, ordenações e transformações, entre outros.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Métodos Select e Collect\n",
    "\n",
    "> Os métodos select e collect permitem a manipulação e coleta de dados em DataFrames do Spark. Abaixo são apresentados diversos exemplos de como esses métodos podem ser utilizados para selecionar colunas, renomear colunas, acessar dados por índice e expressões regulares, entre outras operações.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "### Método select()\n",
    "\n",
    "- **Tipo**: Transformação\n",
    "- **Descrição**: O método select() é utilizado para selecionar colunas específicas de um DataFrame. Ele retorna um novo DataFrame contendo apenas as colunas selecionadas, sem modificar o DataFrame original.\n",
    "- **Uso**: select() é ideal para manipular e filtrar as colunas de interesse em um DataFrame antes de realizar outras operações de transformação ou ação.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exemplos de Uso de select()\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Selecionando dados de 2 colunas (3 formas diferentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "466bff9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|id_veiculo|  entrega|\n",
      "+----------+---------+\n",
      "|       298|Entrega 1|\n",
      "|       298|Entrega 2|\n",
      "|       298|Entrega 3|\n",
      "|       298|Entrega 4|\n",
      "|       298|Entrega 5|\n",
      "+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecionando dados de 2 colunas (1)\n",
    "df.select('id_veiculo', 'entrega').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a51cbe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|id_veiculo|  entrega|\n",
      "+----------+---------+\n",
      "|       298|Entrega 1|\n",
      "|       298|Entrega 2|\n",
      "|       298|Entrega 3|\n",
      "|       298|Entrega 4|\n",
      "|       298|Entrega 5|\n",
      "+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternativamente, podemos usar esta notação (2)\n",
    "df.select(df.id_veiculo, df.entrega).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c054b126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|id_veiculo|  entrega|\n",
      "+----------+---------+\n",
      "|       298|Entrega 1|\n",
      "|       298|Entrega 2|\n",
      "|       298|Entrega 3|\n",
      "|       298|Entrega 4|\n",
      "|       298|Entrega 5|\n",
      "+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A função col é outra alternativa (3)\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col('id_veiculo'), col('entrega')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09effa01",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Podemos selecionar todas as colunas do DataFrame cujos nomes estejam em uma lista (2 formas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd5ae12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|id_veiculo|  entrega|\n",
      "+----------+---------+\n",
      "|       298|Entrega 1|\n",
      "|       298|Entrega 2|\n",
      "|       298|Entrega 3|\n",
      "|       298|Entrega 4|\n",
      "|       298|Entrega 5|\n",
      "+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seleciona todas as colunas do dataframe cujos nomes estejam em uma lista (1)\n",
    "nomes_colunas = [\"id_veiculo\", \"entrega\"]\n",
    "df.select(*nomes_colunas).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b828fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|id_veiculo|  entrega|\n",
      "+----------+---------+\n",
      "|       298|Entrega 1|\n",
      "|       298|Entrega 2|\n",
      "|       298|Entrega 3|\n",
      "|       298|Entrega 4|\n",
      "|       298|Entrega 5|\n",
      "+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mesmo exemplo anterior mas agora com list comprehension (2)\n",
    "df.select([coluna for coluna in nomes_colunas]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e50cc30",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Podemos renomear colunas para facilitar a consulta aos dados (2 formas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c23fc5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|veiculo|  entrega|\n",
      "+-------+---------+\n",
      "|    298|Entrega 1|\n",
      "|    298|Entrega 2|\n",
      "|    298|Entrega 3|\n",
      "|    298|Entrega 4|\n",
      "|    298|Entrega 5|\n",
      "+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renomeando colunas (1)\n",
    "df.select('id_veiculo', 'entrega').withColumnRenamed('id_veiculo', 'veiculo').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa3578a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|veiculo|  entrega|\n",
      "+-------+---------+\n",
      "|    298|Entrega 1|\n",
      "|    298|Entrega 2|\n",
      "|    298|Entrega 3|\n",
      "|    298|Entrega 4|\n",
      "|    298|Entrega 5|\n",
      "+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Também podemos usar um alias para renomear coluna (2)\n",
    "df.select(col('id_veiculo').alias('veiculo'), 'entrega').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899f55a9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Selecionando colunas pelo índice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c29f7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|horario|\n",
      "+-------+\n",
      "|  7:58a|\n",
      "|  8:04a|\n",
      "|  8:17a|\n",
      "+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A partir da coluna de índice 2 retorne as 3 primeiras linhas\n",
    "df.select(df.columns[2:]).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04365c8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Selecionando colunas através de expressões regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e815d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|  entrega|\n",
      "+---------+\n",
      "|Entrega 1|\n",
      "|Entrega 2|\n",
      "|Entrega 3|\n",
      "|Entrega 4|\n",
      "|Entrega 5|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecionando colunas através de expressões regulares\n",
    "# https://docs.python.org/3.9/library/re.html\n",
    "df.select(df.colRegex(\"`^.*Entrega*`\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28ecad",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### O DataFrame original segue intacto\n",
    "\n",
    "- O DataFrame original df permanece inalterado durante todo o processo. As transformações são usadas para criar novos DataFrames modificados, que são então utilizados para visualização ou outras operações subsequentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5308c3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       298|Entrega 1|  7:58a|\n",
      "|       298|Entrega 2|  8:04a|\n",
      "|       298|Entrega 3|  8:17a|\n",
      "|       298|Entrega 4|  8:28a|\n",
      "|       298|Entrega 5|  8:33a|\n",
      "+----------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# O dataframe original segue intacto\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f51f32b",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### Método collect()\n",
    "\n",
    "- **Tipo**: Ação\n",
    "- **Descrição**: O método collect() é usado para recuperar todos os elementos de um DataFrame (ou RDD) para o nó do driver (master) do cluster Spark. Ele coleta os dados de todos os nós do cluster e os traz para o driver como uma lista de objetos Row.\n",
    "- **Uso**: collect() deve ser utilizado com cautela e geralmente é recomendado para conjuntos de dados pequenos. Isso porque trazer grandes volumes de dados para o driver pode resultar em um erro de falta de memória (OutOfMemoryError). É útil para ações que necessitam acessar os dados no driver para visualização ou processamento adicional em Python.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Exemplos de Uso de collect()\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Retornando cada linha do DataFrame como um objeto do tipo Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3203bee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id_veiculo='298', entrega='Entrega 1', horario='7:58a'),\n",
       " Row(id_veiculo='298', entrega='Entrega 2', horario='8:04a'),\n",
       " Row(id_veiculo='298', entrega='Entrega 3', horario='8:17a'),\n",
       " Row(id_veiculo='298', entrega='Entrega 4', horario='8:28a'),\n",
       " Row(id_veiculo='298', entrega='Entrega 5', horario='8:33a'),\n",
       " Row(id_veiculo='298', entrega='Entrega 6', horario='8:39a'),\n",
       " Row(id_veiculo='298', entrega='Entrega 7', horario='9:07a'),\n",
       " Row(id_veiculo='315', entrega='Entrega 1', horario='6:05a'),\n",
       " Row(id_veiculo='315', entrega='Entrega 2', horario='6:14a'),\n",
       " Row(id_veiculo='315', entrega='Entrega 3', horario='6:24a'),\n",
       " Row(id_veiculo='315', entrega='Entrega 4', horario='6:38a'),\n",
       " Row(id_veiculo='315', entrega='Entrega 5', horario='6:45a'),\n",
       " Row(id_veiculo='315', entrega='Entrega 6', horario='6:56a'),\n",
       " Row(id_veiculo='315', entrega='Entrega 7', horario='7:32a'),\n",
       " Row(id_veiculo='457', entrega='Entrega 1', horario='5:04a'),\n",
       " Row(id_veiculo='457', entrega='Entrega 2', horario='5:13a'),\n",
       " Row(id_veiculo='457', entrega='Entrega 3', horario='5:27a'),\n",
       " Row(id_veiculo='457', entrega='Entrega 4', horario='5:39a'),\n",
       " Row(id_veiculo='457', entrega='Entrega 5', horario='5:47a'),\n",
       " Row(id_veiculo='457', entrega='Entrega 6', horario='6:21a'),\n",
       " Row(id_veiculo='457', entrega='Entrega 7', horario='6:38a')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retornando cada linha do dataframe como um objeto do tipo Row\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9ad1bb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### O método collect() retorna uma lista de linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55d00aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando tipo do objeto retornado\n",
    "new_df = df.collect()\n",
    "type(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37406b4b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Podemos \"fatiar\" as estruturas retornadas por collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4be58429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7:58a'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Neste caso retornamos o elemento da primeira linha e terceira coluna\n",
    "df.collect()[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec8ba4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Como collect() retorna uma lista, podemos percorrer a lista com um loop e concatenar as colunas, por exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f6bc89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298,Entrega 1\n",
      "298,Entrega 2\n",
      "298,Entrega 3\n",
      "298,Entrega 4\n",
      "298,Entrega 5\n",
      "298,Entrega 6\n",
      "298,Entrega 7\n",
      "315,Entrega 1\n",
      "315,Entrega 2\n",
      "315,Entrega 3\n",
      "315,Entrega 4\n",
      "315,Entrega 5\n",
      "315,Entrega 6\n",
      "315,Entrega 7\n",
      "457,Entrega 1\n",
      "457,Entrega 2\n",
      "457,Entrega 3\n",
      "457,Entrega 4\n",
      "457,Entrega 5\n",
      "457,Entrega 6\n",
      "457,Entrega 7\n"
     ]
    }
   ],
   "source": [
    "# loop\n",
    "for row in df.collect():\n",
    "    print(row['id_veiculo'] + \",\" + str(row['entrega']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bc225b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Podemos ainda combinar select e collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "004cf14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id_veiculo='298'),\n",
       " Row(id_veiculo='298'),\n",
       " Row(id_veiculo='298'),\n",
       " Row(id_veiculo='298')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtramos colunas com select e filtramos linhas com collect\n",
    "dataCollect = df.select(\"id_veiculo\").collect()[0:4][:]\n",
    "dataCollect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca217d8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Podemos extrair primeiro uma amostra do DataFrame e então coletar o resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "299a3f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id_veiculo='298', entrega='Entrega 2', horario='8:04a'),\n",
       " Row(id_veiculo='298', entrega='Entrega 3', horario='8:17a'),\n",
       " Row(id_veiculo='298', entrega='Entrega 4', horario='8:28a'),\n",
       " Row(id_veiculo='298', entrega='Entrega 6', horario='8:39a'),\n",
       " Row(id_veiculo='298', entrega='Entrega 7', horario='9:07a'),\n",
       " Row(id_veiculo='315', entrega='Entrega 2', horario='6:14a'),\n",
       " Row(id_veiculo='315', entrega='Entrega 4', horario='6:38a'),\n",
       " Row(id_veiculo='457', entrega='Entrega 1', horario='5:04a'),\n",
       " Row(id_veiculo='457', entrega='Entrega 3', horario='5:27a'),\n",
       " Row(id_veiculo='457', entrega='Entrega 6', horario='6:21a'),\n",
       " Row(id_veiculo='457', entrega='Entrega 7', horario='6:38a')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Podemos extrair primeiro uma amostra do dataframe e então coletar o resultado\n",
    "df.sample(0.6).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0441e69e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Resumo:\n",
    "\n",
    "- **select()**: Transformação que retorna um novo DataFrame contendo as colunas selecionadas.\n",
    "- **collect()**: Ação que coleta todos os dados de um DataFrame e os traz para o nó do driver como uma lista de objetos Row. Ideal para pequenos conjuntos de dados.\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "### Métodos Filter e Where\n",
    "\n",
    "> Os métodos filter() e where() são utilizados para filtrar linhas em um DataFrame com base em uma condição especificada. Ambos os métodos funcionam de forma semelhante e são intercambiáveis. Eles retornam um novo DataFrame que contém apenas as linhas que atendem à condição de filtragem, sem modificar o DataFrame original.\n",
    "\n",
    "> Esses métodos são ideais para selecionar subconjuntos de dados que atendem a condições específicas, como valores em determinadas colunas ou expressões booleanas complexas. O DataFrame original df permanece inalterado durante todo o processo. As transformações são usadas para criar novos DataFrames modificados, que são então utilizados para visualização ou outras operações subsequentes.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "### Método filter()\n",
    "\n",
    "- **Tipo**: Transformação\n",
    "- **Descrição**: O método filter() aplica uma condição de filtragem ao DataFrame e retorna um novo DataFrame contendo apenas as linhas que atendem a essa condição. Ele pode utilizar expressões SQL como strings ou expressões de coluna PySpark.\n",
    "- **Uso**: filter() é útil para aplicar condições de filtragem de forma programática, especialmente quando combinada com outras transformações PySpark.\n",
    "\n",
    "### Exemplos de Uso de filter()\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6033c057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       298|Entrega 2|  8:04a|\n",
      "|       315|Entrega 2|  6:14a|\n",
      "|       457|Entrega 2|  5:13a|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Podemos filtrar os dados retornados com a função filter()\n",
    "df_filtered_1 = df.filter(\"entrega == 'Entrega 2'\")\n",
    "df_filtered_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a814030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       298|Entrega 1|  7:58a|\n",
      "|       298|Entrega 3|  8:17a|\n",
      "|       298|Entrega 4|  8:28a|\n",
      "|       298|Entrega 5|  8:33a|\n",
      "|       298|Entrega 6|  8:39a|\n",
      "+----------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Podemos filtrar os dados retornados com a função filter() usando a negação\n",
    "df_filtered_2 = df.filter(\"entrega != 'Entrega 2'\")\n",
    "df_filtered_2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "74afbe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       298|Entrega 2|  8:04a|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Podemos filtrar usando múltiplas condições\n",
    "df_filtered_3 = df.filter((df.entrega == \"Entrega 2\") & (df.id_veiculo == 298))\n",
    "df_filtered_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a020353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       298|Entrega 1|  7:58a|\n",
      "|       298|Entrega 2|  8:04a|\n",
      "|       298|Entrega 3|  8:17a|\n",
      "|       298|Entrega 4|  8:28a|\n",
      "|       298|Entrega 5|  8:33a|\n",
      "|       298|Entrega 6|  8:39a|\n",
      "|       298|Entrega 7|  9:07a|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtro baseado em lista\n",
    "lista_id_veiculos = [298, 300, 400]\n",
    "\n",
    "df_filtered_4 = df.filter(df.id_veiculo.isin(lista_id_veiculos))\n",
    "df_filtered_4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9d127d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       315|Entrega 4|  6:38a|\n",
      "|       457|Entrega 7|  6:38a|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alguma entrega ocorreu no minuto 38 de qualquer hora?\n",
    "df_filtered_5 = df.filter(df.horario.like(\"%38%\"))\n",
    "df_filtered_5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5884c3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Método where()\n",
    "\n",
    "- **Tipo**: Transformação\n",
    "- **Descrição**: O método where() é funcionalmente equivalente ao método filter(), aplicando uma condição de filtragem ao DataFrame e retornando um novo DataFrame contendo apenas as linhas que atendem a essa condição. Ele aceita as mesmas expressões SQL como strings ou expressões de coluna PySpark.\n",
    "- **Uso**: where() é frequentemente usado em cenários onde a sintaxe SQL é mais intuitiva ou preferida.\n",
    "    \n",
    "<br>\n",
    "\n",
    "### Exemplos de Uso de where()\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c80539d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       298|Entrega 2|  8:04a|\n",
      "|       315|Entrega 2|  6:14a|\n",
      "|       457|Entrega 2|  5:13a|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Podemos filtrar os dados retornados com a função where() também\n",
    "df_where_1 = df.where(\"entrega == 'Entrega 2'\")\n",
    "df_where_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85bc9fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       457|Entrega 1|  5:04a|\n",
      "|       457|Entrega 2|  5:13a|\n",
      "|       457|Entrega 3|  5:27a|\n",
      "|       457|Entrega 4|  5:39a|\n",
      "|       457|Entrega 5|  5:47a|\n",
      "|       457|Entrega 6|  6:21a|\n",
      "|       457|Entrega 7|  6:38a|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Podemos filtrar os dados retornados com a função where()\n",
    "df_where_2 = df.where(\"id_veiculo > 400\")\n",
    "df_where_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "586bde03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       298|Entrega 1|  7:58a|\n",
      "|       298|Entrega 2|  8:04a|\n",
      "|       298|Entrega 3|  8:17a|\n",
      "|       298|Entrega 4|  8:28a|\n",
      "|       298|Entrega 5|  8:33a|\n",
      "|       298|Entrega 6|  8:39a|\n",
      "|       298|Entrega 7|  9:07a|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Where baseado em lista\n",
    "lista_id_veiculos = [298, 300, 400]\n",
    "\n",
    "df_where_3 = df.where(df.id_veiculo.isin(lista_id_veiculos))\n",
    "df_where_3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c30fc3",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### Métodos Order By e Sort\n",
    "\n",
    "> Os métodos orderBy() e sort() são utilizados para ordenar as linhas de um DataFrame com base em uma ou mais colunas. Ambos os métodos são funcionalmente equivalentes e podem ser usados para especificar a ordem de classificação de forma ascendente ou descendente.\n",
    "\n",
    "> Esses métodos são ideais para organizar os dados em uma ordem específica antes de realizar operações subsequentes ou visualizar os dados ordenados. O DataFrame original df permanece inalterado durante todo o processo. As transformações são usadas para criar novos DataFrames modificados, que são então utilizados para visualização ou outras operações subsequentes.\n",
    "\n",
    "#### Métodos orderBy() e sort()\n",
    "\n",
    "- **Tipo**: Transformação\n",
    "- **Descrição**: Os métodos orderBy() e sort() são utilizados para ordenar as linhas de um DataFrame com base em uma ou mais colunas. Eles retornam um novo DataFrame com as linhas ordenadas de acordo com a ordem especificada.\n",
    "- **Uso**: Esses métodos são úteis para organizar os dados antes de realizar outras operações ou visualizar os resultados de forma ordenada.\n",
    "    \n",
    "<br>\n",
    "\n",
    "### Exemplos de Uso de orderBy() e sort()\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84b1c3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       457|Entrega 1|  5:04a|\n",
      "|       457|Entrega 2|  5:13a|\n",
      "|       457|Entrega 3|  5:27a|\n",
      "|       457|Entrega 4|  5:39a|\n",
      "|       457|Entrega 5|  5:47a|\n",
      "+----------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ordenando a seleção das linhas\n",
    "df_sorted_1 = df.sort(\"horario\", \"entrega\")\n",
    "df_sorted_1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "49c82a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       457|Entrega 1|  5:04a|\n",
      "|       457|Entrega 2|  5:13a|\n",
      "|       457|Entrega 3|  5:27a|\n",
      "|       457|Entrega 4|  5:39a|\n",
      "|       457|Entrega 5|  5:47a|\n",
      "+----------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mesmo resultado anterior mas com a função col()\n",
    "df_sorted_2 = df.sort(col(\"horario\"), col(\"entrega\"))\n",
    "df_sorted_2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5e29cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       457|Entrega 1|  5:04a|\n",
      "|       457|Entrega 2|  5:13a|\n",
      "|       457|Entrega 3|  5:27a|\n",
      "|       457|Entrega 4|  5:39a|\n",
      "|       457|Entrega 5|  5:47a|\n",
      "+----------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ou usamos orderBy\n",
    "df_sorted_3 = df.orderBy(col(\"horario\"), col(\"entrega\"))\n",
    "df_sorted_3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "30443be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       298|Entrega 7|  9:07a|\n",
      "|       298|Entrega 6|  8:39a|\n",
      "|       298|Entrega 5|  8:33a|\n",
      "|       298|Entrega 4|  8:28a|\n",
      "|       298|Entrega 3|  8:17a|\n",
      "+----------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ordena o resultado em ordem decrescente \n",
    "df_sorted_4 = df.sort(df.horario.desc(), df.entrega.desc())\n",
    "df_sorted_4.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "afb361d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       298|Entrega 7|  9:07a|\n",
      "|       298|Entrega 6|  8:39a|\n",
      "|       298|Entrega 5|  8:33a|\n",
      "|       298|Entrega 4|  8:28a|\n",
      "|       298|Entrega 3|  8:17a|\n",
      "+----------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lembre-se que podemos usar SQL\n",
    "spark.sql(\"select id_veiculo, entrega, horario from tb_logistica ORDER BY horario desc\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22593bf",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### Métodos map(), flatMap() e explode()\n",
    "\n",
    "> Os métodos map(), flatMap() e explode() são usados para transformar e manipular os dados em um DataFrame ou RDD. Abaixo estão exemplos de como utilizar cada um desses métodos em PySpark.\n",
    "\n",
    "> Esses métodos são ideais para realizar transformações complexas, desmembrar elementos ou manipular listas dentro de colunas. O DataFrame original df permanece inalterado durante todo o processo. As transformações são usadas para criar novos DataFrames ou RDDs modificados, que são então utilizados para visualização ou outras operações subsequentes. \n",
    "\n",
    "<br>\n",
    "\n",
    "### Método map()\n",
    "\n",
    "- **Tipo**: Transformação\n",
    "- **Descrição**: O método map() é usado para aplicar uma função a cada elemento de um RDD, retornando um novo RDD com os resultados da função aplicada.\n",
    "- **Uso**: map() é útil quando se deseja transformar cada elemento de um RDD de forma independente.\n",
    "    \n",
    "<br>\n",
    "\n",
    "### Exemplos de Uso de map()\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "979a87e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|      novo_id|entrega|\n",
      "+-------------+-------+\n",
      "|298,Entrega 1|  7:58a|\n",
      "|298,Entrega 2|  8:04a|\n",
      "|298,Entrega 3|  8:17a|\n",
      "|298,Entrega 4|  8:28a|\n",
      "|298,Entrega 5|  8:33a|\n",
      "+-------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Maps são aplicados em RDDs e por isso precisamos converter o dataframe para RDD\n",
    "rdd2 = df.rdd.map(lambda x: (x[0] + \",\" + x[1], x[2]))  \n",
    "\n",
    "# O método Map retorna um RDD e por isso temos que converter de volta para dataframe\n",
    "df2 = rdd2.toDF([\"novo_id\", \"entrega\"])\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ef7a76c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|      novo_id|entrega|\n",
      "+-------------+-------+\n",
      "|298,Entrega 1|  7:58a|\n",
      "|298,Entrega 2|  8:04a|\n",
      "|298,Entrega 3|  8:17a|\n",
      "|298,Entrega 4|  8:28a|\n",
      "|298,Entrega 5|  8:33a|\n",
      "+-------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mesmo exemplo anterior mas usando o nome da coluna e não o índice\n",
    "rdd2 = df.rdd.map(lambda x: (x['id_veiculo'] + \",\" + x['entrega'], x['horario']))  \n",
    "df2 = rdd2.toDF([\"novo_id\", \"entrega\"])\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e058cb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('298-Entrega 1', '7:58a'),\n",
       " ('298-Entrega 2', '8:04a'),\n",
       " ('298-Entrega 3', '8:17a'),\n",
       " ('298-Entrega 4', '8:28a'),\n",
       " ('298-Entrega 5', '8:33a'),\n",
       " ('298-Entrega 6', '8:39a'),\n",
       " ('298-Entrega 7', '9:07a'),\n",
       " ('315-Entrega 1', '6:05a'),\n",
       " ('315-Entrega 2', '6:14a'),\n",
       " ('315-Entrega 3', '6:24a'),\n",
       " ('315-Entrega 4', '6:38a'),\n",
       " ('315-Entrega 5', '6:45a'),\n",
       " ('315-Entrega 6', '6:56a'),\n",
       " ('315-Entrega 7', '7:32a'),\n",
       " ('457-Entrega 1', '5:04a'),\n",
       " ('457-Entrega 2', '5:13a'),\n",
       " ('457-Entrega 3', '5:27a'),\n",
       " ('457-Entrega 4', '5:39a'),\n",
       " ('457-Entrega 5', '5:47a'),\n",
       " ('457-Entrega 6', '6:21a'),\n",
       " ('457-Entrega 7', '6:38a')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando uma função que manipula as colunas\n",
    "def manipula_colunas(x):\n",
    "    coluna1 = x.id_veiculo\n",
    "    coluna2 = x.entrega\n",
    "    novo_id = coluna1 + \"-\" + coluna2\n",
    "    coluna3 = x.horario\n",
    "    return (novo_id, coluna3)\n",
    "\n",
    "# Usamos a função map para aplicar a função lambda, que aplica a função manipula_colunas a cada linha do RDD\n",
    "rdd2 = df.rdd.map(lambda x: manipula_colunas(x))\n",
    "\n",
    "# Collect no RDD\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2921ce",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### Método flatMap()\n",
    "\n",
    "- **Tipo**: Transformação\n",
    "- **Descrição**: O método flatMap() é semelhante ao map(), mas cada elemento do RDD pode ser mapeado para zero ou mais elementos, resultando em um RDD \"achatado\" (flat).\n",
    "- **Uso**: é útil para dividir elementos em vários elementos ou desmembrar listas.\n",
    "    \n",
    "<br>\n",
    "\n",
    "### Exemplos de Uso de flatMap()\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "00ff7d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O método flatMap requer uma lista no formato RDD\n",
    "# Vamos criar uma lista\n",
    "data = [\"A Data Science Academy\",\n",
    "        \"oferece cursos realmente incríveis\",\n",
    "        \"orientados às necessidades\",\n",
    "        \"do mercado de trabalho\",\n",
    "        \"e tudo mostrado passo a passo\"]\n",
    "\n",
    "# Convertemos a lista em um RDD\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6a1ec9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Data Science Academy\n",
      "oferece cursos realmente incríveis\n",
      "orientados às necessidades\n",
      "do mercado de trabalho\n",
      "e tudo mostrado passo a passo\n"
     ]
    }
   ],
   "source": [
    "# Imprime os elementos do RDD\n",
    "for element in rdd.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "77c15f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "Data\n",
      "Science\n",
      "Academy\n",
      "oferece\n",
      "cursos\n",
      "realmente\n",
      "incríveis\n",
      "orientados\n",
      "às\n",
      "necessidades\n",
      "do\n",
      "mercado\n",
      "de\n",
      "trabalho\n",
      "e\n",
      "tudo\n",
      "mostrado\n",
      "passo\n",
      "a\n",
      "passo\n"
     ]
    }
   ],
   "source": [
    "# Agora aplicamos o flatMap, que cria outro RDD  \n",
    "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "# Imprime os elementos do RDD\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f77416f",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### Método explode()\n",
    "\n",
    "- **Tipo**: Transformação\n",
    "- **Descrição**: O método explode() é usado para transformar uma coluna contendo listas ou arrays em várias linhas, criando uma nova linha para cada elemento da lista.\n",
    "- **Uso**: explode() é útil para lidar com colunas que contêm listas e transformar esses dados em um formato mais plano.\n",
    "    \n",
    "<br>\n",
    "\n",
    "### Exemplos de Uso de explode()\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f0e03419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "## Explode deve receber uma lista como argumento, mas essa lista pode estar em uma coluna de um dataframe\n",
    "\n",
    "# Cria uma lista\n",
    "array_estudantes = [('Bob', ['Python', 'R', 'Scala']),\n",
    "                    ('Maria', ['Java','Julia']),\n",
    "                    ('Zico', ['JavaScript', '']),\n",
    "                    ('Ana', [None, None])]\n",
    "\n",
    "type(array_estudantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d25e2963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- aluno: string (nullable = true)\n",
      " |-- col: string (nullable = true)\n",
      "\n",
      "+-----+----------+\n",
      "|aluno|       col|\n",
      "+-----+----------+\n",
      "|  Bob|    Python|\n",
      "|  Bob|         R|\n",
      "|  Bob|     Scala|\n",
      "|Maria|      Java|\n",
      "|Maria|     Julia|\n",
      "| Zico|JavaScript|\n",
      "| Zico|          |\n",
      "|  Ana|      NULL|\n",
      "|  Ana|      NULL|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converte a lista para dataframe\n",
    "df_estudantes = spark.createDataFrame(data = array_estudantes, schema = ['aluno', 'linguagem'])\n",
    "\n",
    "# Select com explode\n",
    "df2 = df_estudantes.select(df_estudantes.aluno, explode(df_estudantes.linguagem))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2403ffdc",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### Método foreach()\n",
    "\n",
    "> O método foreach em PySpark é usado para aplicar uma função a cada elemento de um RDD ou DataFrame. Ao contrário dos métodos de transformação, foreach é uma ação e não retorna um novo RDD ou DataFrame, mas executa a função fornecida em cada elemento do RDD/DataFrame.\n",
    "\n",
    "- **Tipo**: Ação\n",
    "- **Descrição**: Aplica uma função a cada elemento do DataFrame (ou RDD). A função especificada é executada em cada elemento do DataFrame, mas os resultados não são coletados ou retornados.\n",
    "- **Uso**: O método foreach é útil para operações onde o objetivo é executar uma ação em cada elemento, como escrever em um banco de dados, atualizar um sistema externo ou simplesmente imprimir os elementos. No entanto, deve-se ter cuidado ao usar foreach para operações que afetam o estado global, pois o PySpark executa essas funções em paralelo em diferentes nós do cluster.\n",
    "    \n",
    "<br>\n",
    "\n",
    "### Exemplos de Uso de foreach()\n",
    "\n",
    "<br>\n",
    "\n",
    "Neste exemplo, a função lambda concatena os valores das colunas id_veiculo, entrega e horario em uma única string e imprime essa string. O método foreach aplica essa função a cada linha do DataFrame df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ee96da4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "298,Entrega 1,7:58a\n",
      "298,Entrega 2,8:04a\n",
      "298,Entrega 3,8:17a\n",
      "298,Entrega 4,8:28a\n",
      "298,Entrega 5,8:33a\n",
      "298,Entrega 6,8:39a\n",
      "298,Entrega 7,9:07a\n",
      "315,Entrega 1,6:05a\n",
      "315,Entrega 2,6:14a\n",
      "315,Entrega 3,6:24a\n",
      "315,Entrega 4,6:38a\n",
      "315,Entrega 5,6:45a\n",
      "315,Entrega 6,6:56a\n",
      "315,Entrega 7,7:32a\n",
      "457,Entrega 1,5:04a\n",
      "457,Entrega 2,5:13a\n",
      "457,Entrega 3,5:27a\n",
      "457,Entrega 4,5:39a\n",
      "457,Entrega 5,5:47a\n",
      "457,Entrega 6,6:21a\n",
      "457,Entrega 7,6:38a\n"
     ]
    }
   ],
   "source": [
    "# Aplica uma função lambda para imprimir cada linha do DataFrame\n",
    "df.foreach(lambda x: print(x[\"id_veiculo\"] + \",\" + x[\"entrega\"] + \",\" + x[\"horario\"])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a778a2f1",
   "metadata": {},
   "source": [
    "### Resumo\n",
    "\n",
    "- Neste exemplo, foreach é utilizado para imprimir os valores de cada linha do DataFrame. Cada linha é formatada como uma string com os valores das colunas separadas por vírgulas.\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "# Agregação com Spark SQL\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Agregação com Funções do Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "25fbd657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id_veiculo', 'entrega', 'horario']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Colunas do DataFrame\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "28495d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tipo\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5414fb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|id_veiculo|count|\n",
      "+----------+-----+\n",
      "|       315|    7|\n",
      "|       457|    7|\n",
      "|       298|    7|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contagem de entregas por id_veiculo\n",
    "df.groupBy(\"id_veiculo\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa6d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A linha de código abaixo não funciona. LEIA A MENSAGEM DE ERRO!!!\n",
    "# df.groupBy(\"id_veiculo\").min(\"horario\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e2a6887d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|id_veiculo|min(horario)|\n",
      "+----------+------------+\n",
      "|       298|       7:58a|\n",
      "|       315|       6:05a|\n",
      "|       457|       5:04a|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agregação mínima de horário por id_veiculo\n",
    "df.groupBy('id_veiculo').agg({'horario':'min'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "06c626c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|id_veiculo|max(horario)|\n",
      "+----------+------------+\n",
      "|       298|       9:07a|\n",
      "|       315|       7:32a|\n",
      "|       457|       6:38a|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agregação máxima de horário por id_veiculo\n",
    "df.groupBy('id_veiculo').agg({'horario':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e6b13b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|id_veiculo|count(horario)|\n",
      "+----------+--------------+\n",
      "|       315|             7|\n",
      "|       457|             7|\n",
      "|       298|             7|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contagem de horários por id_veiculo\n",
    "df.groupBy('id_veiculo').agg({'horario':'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0bb229bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|id_veiculo|numero_entregas|\n",
      "+----------+---------------+\n",
      "|       315|              7|\n",
      "|       457|              7|\n",
      "|       298|              7|\n",
      "+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renomeando a coluna de contagem de horários para numero_entregas\n",
    "df.groupBy('id_veiculo').agg({'horario':'count'}).withColumnRenamed('count(horario)', 'numero_entregas').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "911886d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+\n",
      "|id_veiculo|hora_primeira_entrega|\n",
      "+----------+---------------------+\n",
      "|       298|                7:58a|\n",
      "|       315|                6:05a|\n",
      "|       457|                5:04a|\n",
      "+----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renomeando a coluna de mínimo horário para hora_primeira_entrega\n",
    "df_novo = df.groupBy('id_veiculo').agg({'horario':'min'}).withColumnRenamed(\n",
    "    'min(horario)', 'hora_primeira_entrega')\n",
    "\n",
    "df_novo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e03fddbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       298|Entrega 1|  7:58a|\n",
      "|       298|Entrega 2|  8:04a|\n",
      "+----------+---------+-------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+---------------------+\n",
      "|id_veiculo|hora_primeira_entrega|\n",
      "+----------+---------------------+\n",
      "|       298|                7:58a|\n",
      "|       315|                6:05a|\n",
      "|       457|                5:04a|\n",
      "+----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizando dataframe original e modificado acima\n",
    "df.show(2)\n",
    "df_novo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab9cff8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Agregação com Queries SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "65a4dc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|id_veiculo|numero_entregas|\n",
      "+----------+---------------+\n",
      "|       315|              7|\n",
      "|       457|              7|\n",
      "|       298|              7|\n",
      "+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usando função SQL do SparkSQL para contagem de entregas por id_veiculo\n",
    "df.groupBy(\"id_veiculo\").count().withColumnRenamed('count', 'numero_entregas').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0125dc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|id_veiculo|numero_entregas|\n",
      "+----------+---------------+\n",
      "|       315|              7|\n",
      "|       457|              7|\n",
      "|       298|              7|\n",
      "+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a query para contar número de entregas por id_veiculo\n",
    "query = \"\"\"\n",
    "SELECT id_veiculo, COUNT(*) AS numero_entregas\n",
    "FROM tb_logistica\n",
    "GROUP BY id_veiculo\n",
    "\"\"\"\n",
    "# Executa a query\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6538406f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+-------------------+\n",
      "|id_veiculo|hora_primeira_entrega|hora_ultima_entrega|\n",
      "+----------+---------------------+-------------------+\n",
      "|       298|                7:58a|              9:07a|\n",
      "|       315|                6:05a|              7:32a|\n",
      "|       457|                5:04a|              6:38a|\n",
      "+----------+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a query para obter o primeiro e último horário de entrega por id_veiculo\n",
    "query = \"\"\"\n",
    "SELECT id_veiculo, MIN(horario) AS hora_primeira_entrega, MAX(horario) AS hora_ultima_entrega\n",
    "FROM tb_logistica\n",
    "GROUP BY id_veiculo\n",
    "\"\"\"\n",
    "# Executa a query\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1c089305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|horario|hora_ultima_entrega|\n",
      "+-------+-------------------+\n",
      "|  8:04a|                  1|\n",
      "|  8:28a|                  1|\n",
      "|  8:39a|                  1|\n",
      "|  9:07a|                  1|\n",
      "|  7:58a|                  1|\n",
      "|  8:17a|                  1|\n",
      "|  8:33a|                  1|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a query para contar o número de entregas por horário para o id_veiculo 298\n",
    "query = \"\"\"\n",
    "SELECT horario, COUNT(*) AS hora_ultima_entrega\n",
    "FROM tb_logistica\n",
    "WHERE id_veiculo = 298\n",
    "GROUP BY horario\n",
    "\"\"\"\n",
    "# Executa a query\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "79143136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|horario|numero_entregas|\n",
      "+-------+---------------+\n",
      "|  8:04a|              1|\n",
      "|  8:28a|              1|\n",
      "|  8:39a|              1|\n",
      "|  9:07a|              1|\n",
      "|  6:05a|              1|\n",
      "|  6:24a|              1|\n",
      "|  6:38a|              2|\n",
      "|  6:45a|              1|\n",
      "|  7:32a|              1|\n",
      "|  5:04a|              1|\n",
      "|  5:13a|              1|\n",
      "|  7:58a|              1|\n",
      "|  8:17a|              1|\n",
      "|  8:33a|              1|\n",
      "|  6:14a|              1|\n",
      "|  6:56a|              1|\n",
      "|  5:27a|              1|\n",
      "|  5:39a|              1|\n",
      "|  5:47a|              1|\n",
      "|  6:21a|              1|\n",
      "+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a query para contar o número de entregas por horário\n",
    "query = \"\"\"\n",
    "SELECT horario, COUNT(*) AS numero_entregas\n",
    "FROM tb_logistica\n",
    "GROUP BY horario\n",
    "\"\"\"\n",
    "# Executa a query\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "962f84a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|horario|hora_ultima_entrega|\n",
      "+-------+-------------------+\n",
      "|  6:38a|                  2|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a query para identificar horários que tiveram mais de uma entrega\n",
    "query = \"\"\"\n",
    "SELECT horario, COUNT(*) AS hora_ultima_entrega\n",
    "FROM tb_logistica\n",
    "GROUP BY horario\n",
    "HAVING COUNT(*) > 1\n",
    "\"\"\"\n",
    "# Executa a query\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d72e34",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Fazendo um Pivot de um DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e535a112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|id_veiculo|  entrega|horario|\n",
      "+----------+---------+-------+\n",
      "|       298|Entrega 2|  8:04a|\n",
      "|       298|Entrega 7|  9:07a|\n",
      "|       315|Entrega 4|  6:38a|\n",
      "|       315|Entrega 7|  7:32a|\n",
      "|       457|Entrega 2|  5:13a|\n",
      "|       457|Entrega 7|  6:38a|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lista de horários de entregas\n",
    "lista_horarios = ['5:13a', '6:38a', '7:32a', '8:04a', '9:07a']\n",
    "\n",
    "# Testamos o filtro\n",
    "df.filter(df.horario.isin(lista_horarios)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "974c65d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+-----+\n",
      "|id_veiculo|5:13a|6:38a|7:32a|8:04a|9:07a|\n",
      "+----------+-----+-----+-----+-----+-----+\n",
      "|       315| NULL|    1|    1| NULL| NULL|\n",
      "|       457|    1|    1| NULL| NULL| NULL|\n",
      "|       298| NULL| NULL| NULL|    1|    1|\n",
      "+----------+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pivot é uma função de agregação no Spark\n",
    "df_pivot = df.filter(df.horario.isin(lista_horarios)).groupBy(\"id_veiculo\").pivot(\"horario\").count()\n",
    "df_pivot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c0623707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_veiculo</th>\n",
       "      <th>5:13a</th>\n",
       "      <th>6:38a</th>\n",
       "      <th>7:32a</th>\n",
       "      <th>8:04a</th>\n",
       "      <th>9:07a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>457</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_veiculo  5:13a  6:38a  7:32a  8:04a  9:07a\n",
       "0        315    NaN    1.0    1.0    NaN    NaN\n",
       "1        457    1.0    1.0    NaN    NaN    NaN\n",
       "2        298    NaN    NaN    NaN    1.0    1.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converte o Spark DataFrame para Pandas DataFrame a fim de facilitar a visualização\n",
    "pandasDF = df_pivot.toPandas()\n",
    "pandasDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9ab64720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+-----+\n",
      "|id_veiculo|5:13a|6:38a|7:32a|8:04a|9:07a|\n",
      "+----------+-----+-----+-----+-----+-----+\n",
      "|       298| NULL| NULL| NULL|    1|    1|\n",
      "|       315| NULL|    1|    1| NULL| NULL|\n",
      "|       457|    1|    1| NULL| NULL| NULL|\n",
      "+----------+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a query para criar um pivot\n",
    "query = \"\"\"\n",
    "SELECT * FROM (\n",
    "  SELECT id_veiculo, horario\n",
    "  FROM tb_logistica\n",
    ")\n",
    "PIVOT (\n",
    "  COUNT(*)\n",
    "  FOR horario in (\n",
    "    '5:13a', '6:38a', '7:32a', '8:04a', '9:07a'\n",
    "  )\n",
    ")\n",
    "ORDER BY id_veiculo\n",
    "\"\"\"\n",
    "# Executa a query\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f13da",
   "metadata": {},
   "source": [
    "### Resumo\n",
    "\n",
    "- Os exemplos acima mostram como utilizar as funções de agregação e consultas SQL no PySpark para realizar operações complexas e transformar os dados de várias formas. O DataFrame original df permanece inalterado durante todo o processo, enquanto novas transformações são aplicadas para criar DataFrames modificados para visualização ou operações subsequentes.\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "# SQL Window Function Para Agregação ao Longo do Tempo\n",
    "\n",
    "- Quando usamos a cláusula GROUP BY em SQL, nosso objetivo é fazer agregações no nível de coluna. Por exemplo, considere a tabela tb_funcionarios abaixo:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>ID_Funcionario</th>\n",
    "    <th>Nome_Funcionario</th>\n",
    "    <th>Departamento</th>\n",
    "    <th>Salario (R$)</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1001</td>\n",
    "    <td>Bob</td>\n",
    "    <td>Marketing</td>\n",
    "    <td>8.000,00</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1002</td>\n",
    "    <td>Zico</td>\n",
    "    <td>Finanças</td>\n",
    "    <td>7.500,00</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1003</td>\n",
    "    <td>Bete</td>\n",
    "    <td>Marketing</td>\n",
    "    <td>8.200,00</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1004</td>\n",
    "    <td>Josias</td>\n",
    "    <td>RH</td>\n",
    "    <td>6.000,00</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1005</td>\n",
    "    <td>Maria</td>\n",
    "    <td>RH</td>\n",
    "    <td>6.700,00</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "\n",
    "Se quisermos calcular a média de salário por departamento, a query SQL seria:\n",
    "\n",
    "**SELECT departamento, AVG(salario)**<br>\n",
    "**FROM tb_funcionarios**<br>\n",
    "**GROUP BY departamento;**\n",
    "\n",
    "Essa query agrupa os dados por departamento e calcula a média dos salários. Podemos usar outras funções de agregação como SUM(), MIN(), MAX() e COUNT().\n",
    "\n",
    "<br>\n",
    "\n",
    "> No entanto, quando precisamos fazer agregações no nível de linha, seguindo ou não uma ordem temporal, usamos funções Window. Elas permitem calcular resultados em um intervalo de linhas, proporcionando uma forma mais granular de agregação.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Exemplo de Agregação por Coluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2fb0873e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|id_veiculo|numero_entregas|\n",
      "+----------+---------------+\n",
      "|       315|              7|\n",
      "|       457|              7|\n",
      "|       298|              7|\n",
      "+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agregação por coluna usando GROUP BY\n",
    "query = \"\"\"\n",
    "SELECT id_veiculo, COUNT(horario) AS numero_entregas\n",
    "FROM tb_logistica\n",
    "GROUP BY id_veiculo\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c21166",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Exemplo de Agregação por Linha usando Funções Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b3c80d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+-------+\n",
      "|id_veiculo|  entrega|horario|ranking|\n",
      "+----------+---------+-------+-------+\n",
      "|       457|Entrega 1|  5:04a|      1|\n",
      "|       315|Entrega 1|  6:05a|      2|\n",
      "|       298|Entrega 1|  7:58a|      3|\n",
      "|       457|Entrega 2|  5:13a|      1|\n",
      "|       315|Entrega 2|  6:14a|      2|\n",
      "|       298|Entrega 2|  8:04a|      3|\n",
      "|       457|Entrega 3|  5:27a|      1|\n",
      "|       315|Entrega 3|  6:24a|      2|\n",
      "|       298|Entrega 3|  8:17a|      3|\n",
      "|       457|Entrega 4|  5:39a|      1|\n",
      "|       315|Entrega 4|  6:38a|      2|\n",
      "|       298|Entrega 4|  8:28a|      3|\n",
      "|       457|Entrega 5|  5:47a|      1|\n",
      "|       315|Entrega 5|  6:45a|      2|\n",
      "|       298|Entrega 5|  8:33a|      3|\n",
      "|       457|Entrega 6|  6:21a|      1|\n",
      "|       315|Entrega 6|  6:56a|      2|\n",
      "|       298|Entrega 6|  8:39a|      3|\n",
      "|       457|Entrega 7|  6:38a|      1|\n",
      "|       315|Entrega 7|  7:32a|      2|\n",
      "|       298|Entrega 7|  9:07a|      3|\n",
      "+----------+---------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agregação por linha usando função Window para criar um ranking\n",
    "query = \"\"\"\n",
    "SELECT id_veiculo, entrega, horario,\n",
    "ROW_NUMBER() OVER (PARTITION BY entrega ORDER BY horario) AS ranking\n",
    "FROM tb_logistica\n",
    "\"\"\"\n",
    "spark.sql(query).show(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a6a80d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Usando Funções Window com SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e38e4fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---+\n",
      "|id_veiculo|  entrega|horario| id|\n",
      "+----------+---------+-------+---+\n",
      "|       457|Entrega 1|  5:04a|  1|\n",
      "|       315|Entrega 1|  6:05a|  2|\n",
      "|       298|Entrega 1|  7:58a|  3|\n",
      "|       457|Entrega 2|  5:13a|  1|\n",
      "|       315|Entrega 2|  6:14a|  2|\n",
      "|       298|Entrega 2|  8:04a|  3|\n",
      "|       457|Entrega 3|  5:27a|  1|\n",
      "|       315|Entrega 3|  6:24a|  2|\n",
      "|       298|Entrega 3|  8:17a|  3|\n",
      "|       457|Entrega 4|  5:39a|  1|\n",
      "|       315|Entrega 4|  6:38a|  2|\n",
      "|       298|Entrega 4|  8:28a|  3|\n",
      "|       457|Entrega 5|  5:47a|  1|\n",
      "|       315|Entrega 5|  6:45a|  2|\n",
      "|       298|Entrega 5|  8:33a|  3|\n",
      "|       457|Entrega 6|  6:21a|  1|\n",
      "|       315|Entrega 6|  6:56a|  2|\n",
      "|       298|Entrega 6|  8:39a|  3|\n",
      "|       457|Entrega 7|  6:38a|  1|\n",
      "|       315|Entrega 7|  7:32a|  2|\n",
      "|       298|Entrega 7|  9:07a|  3|\n",
      "+----------+---------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, lag, lead\n",
    "\n",
    "# Criando um ranking com SparkSQL\n",
    "df.withColumn(\"id\", row_number().over(Window.partitionBy('entrega').orderBy('horario'))).show(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e0cf5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Usando LAG e LEAD para Acessar Linhas Anteriores e Posteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "614dd23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+----------------+\n",
      "|id_veiculo|  entrega|horario|entrega_anterior|\n",
      "+----------+---------+-------+----------------+\n",
      "|       298|Entrega 1|  7:58a|            NULL|\n",
      "|       298|Entrega 2|  8:04a|           7:58a|\n",
      "|       298|Entrega 3|  8:17a|           8:04a|\n",
      "|       298|Entrega 4|  8:28a|           8:17a|\n",
      "|       298|Entrega 5|  8:33a|           8:28a|\n",
      "|       298|Entrega 6|  8:39a|           8:33a|\n",
      "|       298|Entrega 7|  9:07a|           8:39a|\n",
      "|       315|Entrega 1|  6:05a|            NULL|\n",
      "|       315|Entrega 2|  6:14a|           6:05a|\n",
      "|       315|Entrega 3|  6:24a|           6:14a|\n",
      "|       315|Entrega 4|  6:38a|           6:24a|\n",
      "|       315|Entrega 5|  6:45a|           6:38a|\n",
      "|       315|Entrega 6|  6:56a|           6:45a|\n",
      "|       315|Entrega 7|  7:32a|           6:56a|\n",
      "|       457|Entrega 1|  5:04a|            NULL|\n",
      "|       457|Entrega 2|  5:13a|           5:04a|\n",
      "|       457|Entrega 3|  5:27a|           5:13a|\n",
      "|       457|Entrega 4|  5:39a|           5:27a|\n",
      "|       457|Entrega 5|  5:47a|           5:39a|\n",
      "|       457|Entrega 6|  6:21a|           5:47a|\n",
      "|       457|Entrega 7|  6:38a|           6:21a|\n",
      "+----------+---------+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usando LAG para acessar a entrega anterior\n",
    "query = \"\"\"\n",
    "SELECT id_veiculo, entrega, horario, \n",
    "LAG(horario, 1) OVER (PARTITION BY id_veiculo ORDER BY horario) AS entrega_anterior \n",
    "FROM tb_logistica\n",
    "\"\"\"\n",
    "spark.sql(query).show(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "89e7b30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+\n",
      "|id_veiculo|  entrega|horario|proxima_entrega|\n",
      "+----------+---------+-------+---------------+\n",
      "|       298|Entrega 1|  7:58a|          8:04a|\n",
      "|       298|Entrega 2|  8:04a|          8:17a|\n",
      "|       298|Entrega 3|  8:17a|          8:28a|\n",
      "|       298|Entrega 4|  8:28a|          8:33a|\n",
      "|       298|Entrega 5|  8:33a|          8:39a|\n",
      "|       298|Entrega 6|  8:39a|          9:07a|\n",
      "|       298|Entrega 7|  9:07a|           NULL|\n",
      "|       315|Entrega 1|  6:05a|          6:14a|\n",
      "|       315|Entrega 2|  6:14a|          6:24a|\n",
      "|       315|Entrega 3|  6:24a|          6:38a|\n",
      "|       315|Entrega 4|  6:38a|          6:45a|\n",
      "|       315|Entrega 5|  6:45a|          6:56a|\n",
      "|       315|Entrega 6|  6:56a|          7:32a|\n",
      "|       315|Entrega 7|  7:32a|           NULL|\n",
      "|       457|Entrega 1|  5:04a|          5:13a|\n",
      "|       457|Entrega 2|  5:13a|          5:27a|\n",
      "|       457|Entrega 3|  5:27a|          5:39a|\n",
      "|       457|Entrega 4|  5:39a|          5:47a|\n",
      "|       457|Entrega 5|  5:47a|          6:21a|\n",
      "|       457|Entrega 6|  6:21a|          6:38a|\n",
      "|       457|Entrega 7|  6:38a|           NULL|\n",
      "+----------+---------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usando LEAD para acessar a próxima entrega\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "id_veiculo, entrega, horario, \n",
    "LEAD(horario, 1) OVER (PARTITION BY id_veiculo ORDER BY horario) AS proxima_entrega \n",
    "FROM tb_logistica\n",
    "\"\"\"\n",
    "spark.sql(query).show(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8cef6e",
   "metadata": {},
   "source": [
    "### Resumo\n",
    "\n",
    "- Os exemplos acima mostram como utilizar funções de agregação e consultas SQL no PySpark para realizar operações complexas e transformar dados de várias formas. O DataFrame original df permanece inalterado durante todo o processo, enquanto novas transformações são aplicadas para criar DataFrames modificados para visualização ou operações subsequentes.\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "# Usando Partições com Spark SQL\n",
    "- A função over() no Spark SQL corresponde a cláusula OVER em SQL.\n",
    "- Ela é utilizada para realizar agregações ou operações sobre uma janela de dados especificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1ba490c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.window.WindowSpec"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Abre a janela nos dados\n",
    "janela = Window.partitionBy('id_veiculo').orderBy('horario')\n",
    "type(janela)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bcf30f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+\n",
      "|id_veiculo|  entrega|horario|proxima_entrega|\n",
      "+----------+---------+-------+---------------+\n",
      "|       298|Entrega 1|  7:58a|          8:04a|\n",
      "|       298|Entrega 2|  8:04a|          8:17a|\n",
      "|       298|Entrega 3|  8:17a|          8:28a|\n",
      "|       298|Entrega 4|  8:28a|          8:33a|\n",
      "|       298|Entrega 5|  8:33a|          8:39a|\n",
      "|       298|Entrega 6|  8:39a|          9:07a|\n",
      "|       298|Entrega 7|  9:07a|           NULL|\n",
      "|       315|Entrega 1|  6:05a|          6:14a|\n",
      "|       315|Entrega 2|  6:14a|          6:24a|\n",
      "|       315|Entrega 3|  6:24a|          6:38a|\n",
      "|       315|Entrega 4|  6:38a|          6:45a|\n",
      "|       315|Entrega 5|  6:45a|          6:56a|\n",
      "|       315|Entrega 6|  6:56a|          7:32a|\n",
      "|       315|Entrega 7|  7:32a|           NULL|\n",
      "|       457|Entrega 1|  5:04a|          5:13a|\n",
      "|       457|Entrega 2|  5:13a|          5:27a|\n",
      "|       457|Entrega 3|  5:27a|          5:39a|\n",
      "|       457|Entrega 4|  5:39a|          5:47a|\n",
      "|       457|Entrega 5|  5:47a|          6:21a|\n",
      "|       457|Entrega 6|  6:21a|          6:38a|\n",
      "|       457|Entrega 7|  6:38a|           NULL|\n",
      "+----------+---------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aplica o Lead (desloca os dados no tempo) sobre (over) a janela (window)\n",
    "dfx = df.withColumn('proxima_entrega', lead('horario', 1).over(janela))\n",
    "dfx.show(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6f16791e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+\n",
      "|id_veiculo|  entrega|horario|proxima_entrega|\n",
      "+----------+---------+-------+---------------+\n",
      "|       298|Entrega 1|  7:58a|          8:04a|\n",
      "|       298|Entrega 2|  8:04a|          8:17a|\n",
      "|       298|Entrega 3|  8:17a|          8:28a|\n",
      "|       298|Entrega 4|  8:28a|          8:33a|\n",
      "|       298|Entrega 5|  8:33a|          8:39a|\n",
      "|       298|Entrega 6|  8:39a|          9:07a|\n",
      "|       298|Entrega 7|  9:07a|           NULL|\n",
      "|       315|Entrega 1|  6:05a|          6:14a|\n",
      "|       315|Entrega 2|  6:14a|          6:24a|\n",
      "|       315|Entrega 3|  6:24a|          6:38a|\n",
      "|       315|Entrega 4|  6:38a|          6:45a|\n",
      "|       315|Entrega 5|  6:45a|          6:56a|\n",
      "|       315|Entrega 6|  6:56a|          7:32a|\n",
      "|       315|Entrega 7|  7:32a|           NULL|\n",
      "|       457|Entrega 1|  5:04a|          5:13a|\n",
      "|       457|Entrega 2|  5:13a|          5:27a|\n",
      "|       457|Entrega 3|  5:27a|          5:39a|\n",
      "|       457|Entrega 4|  5:39a|          5:47a|\n",
      "|       457|Entrega 5|  5:47a|          6:21a|\n",
      "|       457|Entrega 6|  6:21a|          6:38a|\n",
      "|       457|Entrega 7|  6:38a|           NULL|\n",
      "+----------+---------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mesmo exemplo anterior, mas em uma linha de código\n",
    "df_dot = df.withColumn('proxima_entrega', lead('horario', 1)\n",
    "                       .over(Window.partitionBy('id_veiculo')\n",
    "                       .orderBy('horario'))).show(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9e19eb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Parse de Data Para Agregação ao Longo do Tempo\n",
    "- Calcule o tempo (em minutos) para a próxima entrega de cada veículo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "577e9458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------------+\n",
      "|id_veiculo|  entrega|horario|tempo_proxima_entrega|\n",
      "+----------+---------+-------+---------------------+\n",
      "|       298|Entrega 1|  7:58a|                  6.0|\n",
      "|       298|Entrega 2|  8:04a|                 13.0|\n",
      "|       298|Entrega 3|  8:17a|                 11.0|\n",
      "|       298|Entrega 4|  8:28a|                  5.0|\n",
      "|       298|Entrega 5|  8:33a|                  6.0|\n",
      "|       298|Entrega 6|  8:39a|                 28.0|\n",
      "|       298|Entrega 7|  9:07a|                 NULL|\n",
      "|       315|Entrega 1|  6:05a|                  9.0|\n",
      "|       315|Entrega 2|  6:14a|                 10.0|\n",
      "|       315|Entrega 3|  6:24a|                 14.0|\n",
      "|       315|Entrega 4|  6:38a|                  7.0|\n",
      "|       315|Entrega 5|  6:45a|                 11.0|\n",
      "|       315|Entrega 6|  6:56a|                 36.0|\n",
      "|       315|Entrega 7|  7:32a|                 NULL|\n",
      "|       457|Entrega 1|  5:04a|                  9.0|\n",
      "|       457|Entrega 2|  5:13a|                 14.0|\n",
      "|       457|Entrega 3|  5:27a|                 12.0|\n",
      "|       457|Entrega 4|  5:39a|                  8.0|\n",
      "|       457|Entrega 5|  5:47a|                 34.0|\n",
      "|       457|Entrega 6|  6:21a|                 17.0|\n",
      "|       457|Entrega 7|  6:38a|                 NULL|\n",
      "+----------+---------+-------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define o time parser policy\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "\n",
    "# Cria a janela\n",
    "window = Window.partitionBy('id_veiculo').orderBy('horario')\n",
    "\n",
    "# Agregação por linha para calcular a diferença entre os horários\n",
    "dot_df = df.withColumn('tempo_proxima_entrega', \n",
    "                       (unix_timestamp(lead('horario', 1).over(window),'H:m') \n",
    "                        - unix_timestamp('horario', 'H:m'))/60).show(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdace698",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Query para calcular a diferença de tempo de uma entrega para outra por id_veiculo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5e861b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------------+\n",
      "|id_veiculo|  entrega|horario|tempo_proxima_entrega|\n",
      "+----------+---------+-------+---------------------+\n",
      "|       298|Entrega 1|  7:58a|                  6.0|\n",
      "|       298|Entrega 2|  8:04a|                 13.0|\n",
      "|       298|Entrega 3|  8:17a|                 11.0|\n",
      "|       298|Entrega 4|  8:28a|                  5.0|\n",
      "|       298|Entrega 5|  8:33a|                  6.0|\n",
      "|       298|Entrega 6|  8:39a|                 28.0|\n",
      "|       298|Entrega 7|  9:07a|                 NULL|\n",
      "|       315|Entrega 1|  6:05a|                  9.0|\n",
      "|       315|Entrega 2|  6:14a|                 10.0|\n",
      "|       315|Entrega 3|  6:24a|                 14.0|\n",
      "|       315|Entrega 4|  6:38a|                  7.0|\n",
      "|       315|Entrega 5|  6:45a|                 11.0|\n",
      "|       315|Entrega 6|  6:56a|                 36.0|\n",
      "|       315|Entrega 7|  7:32a|                 NULL|\n",
      "|       457|Entrega 1|  5:04a|                  9.0|\n",
      "|       457|Entrega 2|  5:13a|                 14.0|\n",
      "|       457|Entrega 3|  5:27a|                 12.0|\n",
      "|       457|Entrega 4|  5:39a|                  8.0|\n",
      "|       457|Entrega 5|  5:47a|                 34.0|\n",
      "|       457|Entrega 6|  6:21a|                 17.0|\n",
      "|       457|Entrega 7|  6:38a|                 NULL|\n",
      "+----------+---------+-------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a query\n",
    "query = \"\"\"\n",
    "SELECT *, \n",
    "(UNIX_TIMESTAMP(LEAD(horario, 1) OVER (PARTITION BY id_veiculo ORDER BY horario),'H:m') \n",
    "- UNIX_TIMESTAMP(horario, 'H:m'))/60 AS tempo_proxima_entrega\n",
    "FROM tb_logistica \n",
    "\"\"\"\n",
    "sql_df = spark.sql(query)\n",
    "sql_df.show(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d8ee3d",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "# FIM!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
