{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b009f1",
   "metadata": {},
   "source": [
    "# <span style=\"color: green; font-size: 40px; font-weight: bold;\">Apache Spark</span>\n",
    "\n",
    "\n",
    "<br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c956ed",
   "metadata": {},
   "source": [
    "# O que é ?\n",
    "\n",
    "- É um <i>framework de processamento de dados distribuído de uso geral</i> e pode ser usado como <i>um framework de análise unificada extremamente rápido para Big Data e Machine Learning</i>.\n",
    "\n",
    "<br>\n",
    "\n",
    "- O Apache Spark permite processar grandes volumes de dados mais rapidamente, dividindo o trabalho em partes e atribuindo essas partes aos recursos computacionais através de máquinas de um cluester de computadores.\n",
    "\n",
    "<br>\n",
    "\n",
    "# Por que Precisamos do Apache Spark?\n",
    "\n",
    "#### A pergunta será respondida através de um <i>Storytelling</i>.\n",
    "\n",
    "<br>\n",
    "\n",
    "Imagine que você tenha que criar um modelo de Machine Learning com uma massa de dados de aproximadamente 10 GB.\n",
    "\n",
    "Você pode usar o Scikit-learn para construir o modelo e realizar todo o processamento em uma única máquina.\n",
    "\n",
    "Caso o próximo volume de dados for de 100GB, talvez seja necessários aumentar o hardware da máquina, adicionando mais CPU, mais memória RAM e mais espaço em disco.\n",
    "\n",
    "<br>\n",
    "\n",
    "O **problema** é que uma única máquina apresenta **escalabilidade vertical**, que é o limite de hardware que você pode adicionar ao computador. **Ou seja**, estaremos limitados à capacidade de um processamento de uma única máquina.<br>\n",
    "\n",
    "#### Como resolver esse problema ?\n",
    "\n",
    "Neste caso poderíamos usar um cluster de computadores, um conjunto de máquinas que vão trabalhar juntas, aumentando assim a capacidade de processamento.\n",
    "\n",
    "Em clusters, temos a escabilidade vertical (aumentando o hardware de cada máquina do cluster), mas temos também a escalabilidade horizontal, que é aumentar o número de máquinas no cluster. O problema agora poderia ser de limitação física para comportar todas essas novas máquinas e com isso teríamos um **outro problema**.\n",
    "\n",
    "O **problema** é que para executar um software em um ambiente de cluster de computadores, o software deve ser capaz de realizar processamento distribuído, ou seja, dividir uma tarefa de processamento em sub-tarefas, enviálas para as máquinas do cluster, coletar as respostas, juntar tudo e entregar o resultado.\n",
    "\n",
    "Toda essa gestão de processamento é feito via software, e por isso precisaríamos de um software que seja capaz de trabalhar de maneira distribuída e o **Scikit-learn não é um framework para procssamento distribuído**, é um framework sequencial.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### E agora por precisamos mesmo do Apache Spark ?\n",
    "\n",
    "> O **Apache Spark** é exatamente para realizar o processamento distribuído e em paralelo.\n",
    "\n",
    "<br>\n",
    "\n",
    "Podemos usar o Apache Spark em uma única máquina (como será feito aqui) ou em um cluster de 6.000 máquinas. A formo como construímos o processo de análise será a mesma!\n",
    "\n",
    "Além disso o Apacha Spark **oferece** bibliotecas poderosas para processamento de Machine Learning, processamento de dados estruturados com SQL, processamento de dados em tempo real com Streaming e processamento de grafos computacionais.\n",
    "\n",
    "O Apache Spark não tem um sistema de armazenamento, pois é um framework de processamento. mas pode ser usado em conjunto com um ambiente de armazenamento distribuído como o **Apache Hadoop HDFS**.\n",
    "\n",
    "Podemos usar o Apache Spark localmente ou na nuvem, como um pseudo-cluster (cluster de uma máquina só) para ambiente de desenvolvimento ou em um cluster de milhares de máquinas em um ambiente de produção.\n",
    "\n",
    "E ainda podemos trabalhar com linguagens Python, Java, Scala ou R\n",
    "\n",
    "<br>\n",
    "\n",
    "**Aqui** usaremos Linguagem Python e pseudo-cluster.\n",
    "\n",
    "\n",
    "<br> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33136783",
   "metadata": {},
   "source": [
    "# Ecossistema e Componentes do Apache Spark\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c44052a",
   "metadata": {},
   "source": [
    "> Na prática o Apache Spark é uma plataforma que possui uma śerie de componentes que podem ser usados em conjunto de acordo com o tipo do projeto. \n",
    "\n",
    "### Spark Core\n",
    "\n",
    "**Spark Core**: é o componente principal do ecossistema do Apache Spark (é o coração do sitema, chamado de motor), considerado mecanismo principal do Apache Spark. O Spark Core inclui o motor (engine) que realiza o processamento de dados distribuído e serve como a base para as outras bibliotecas do Apache Spark.\n",
    "\n",
    "O Spark Core fornece funcionalidades básicas de processamento de dados, incluindo:\n",
    "\n",
    "- Gerenciamento de tarefas de computação distribuída.\n",
    "- Operações básicas de leitura e escrita em fontes de dados.\n",
    "- Funcionalidades de cache e persistência de dados.\n",
    "- Suporte a várias linguagens de programação, como Python, Java, Scala e R.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Principais Bibliotecas do Apache Spark\n",
    "\n",
    "> Acima do Spark Core, existem pelo menos quatro bibliotecas principais que expandem as funcionalidades do Spark, cada uma voltada para um tipo específico de processamento de dados:\n",
    "\n",
    "#### SparkSQL\n",
    "\n",
    "- Biblioteca para processamento de dados estruturados.\n",
    "- Permite executar consultas SQL em grandes conjuntos de dados de maneira distribuída.\n",
    "- Integra-se com várias fontes de dados como Hive, Avro, Parquet, entre outras.\n",
    "\n",
    "#### MLlib\n",
    "\n",
    "- Biblioteca de aprendizado de máquina.\n",
    "- Oferece uma ampla gama de algoritmos de machine learning, incluindo classificação, regressão, clustering e filtragem colaborativa.\n",
    "- Facilita a construção e implementação de modelos de machine learning em larga escala.\n",
    "\n",
    "#### GraphX\n",
    "\n",
    "- Biblioteca para processamento de grafos computacionais.\n",
    "- Permite a análise de grafos e redes sociais.\n",
    "- Oferece algoritmos para computação de grafos, como PageRank, Connected Components e Triangle Counting.\n",
    "\n",
    "#### Streaming\n",
    "\n",
    "- Biblioteca para processamento de dados em tempo real.\n",
    "- Permite o processamento contínuo de dados de fontes de streaming, como Kafka, Flume, e sockets TCP.\n",
    "- Suporta transformações de dados em tempo real e algoritmos de machine learning para streaming.\n",
    "\n",
    "#### Exemplificando\n",
    "\n",
    "- Para **construção de Modelos de Machine Learning** usamos o **MLlib**.\n",
    "- Para **capturar e processar dados em tempo real** usamos o **Streaming**.\n",
    "- Para **capturar dados em tempo real e aplicar Machine Learning** então usaremos **Streaming** juntamente co **MLlib**.\n",
    "- Para **executar uma consulta/query** em grande quantidade conjunto de dados em cluster, então usa o **SparkSQL**.\n",
    "\n",
    "### APIs do Apache Spark\n",
    "\n",
    "> O Apache Spark suporta várias APIs de programação, permitindo que desenvolvedores trabalhem com a linguagem de sua preferência. As principais APIs são:\n",
    "\n",
    "- **Python (PySpark)**: Facilita a integração com bibliotecas Python populares, como pandas e NumPy.\n",
    "- **R**: Usada principalmente para análises estatísticas e gráficos.\n",
    "- **Java**: Ideal para desenvolvedores que trabalham em ecossistemas Java.\n",
    "- **Scala**: A linguagem nativa do Apache Spark, oferece alta performance.\n",
    "- **SQL**: Permite consultas SQL em dados estruturados, aproveitando a familiaridade com SQL.\n",
    "\n",
    "#### Exemplificando\n",
    "\n",
    "Em um projeto, podemos trabalhar com a API **Python (PySpark)** e usar pelo menos uma biblioteca principal do Apache Spark. O uso de PySpark permitirá que aproveitemos a familiaridade com Python, enquanto exploramos as poderosas funcionalidades oferecidas pelas bibliotecas do Spark, como SparkSQL e MLlib, para processamento de dados e machine learning.\n",
    "\n",
    "Por exemplo, podemos usar a API PySpark para carregar dados, transformá-los e executar consultas SQL com SparkSQL, além de aplicar algoritmos de machine learning usando MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1b7787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d8359c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4e8e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30010e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36da43ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
